# title: æµ‹è¯•æ•°æ®åˆ†æ

from datetime import datetime
from pathlib import Path
from typing import List, Optional

import altair as alt
import pandas as pd
import streamlit as st
from utils.compat import inject_structured_clone_polyfill

REPORT_PREFIX = "å¸¸ç”¨æµ‹è¯•æ•°æ®æŠ¥è¡¨"
ALLOWED_EXTENSIONS = (".xlsx", ".xls")
HOME_REPORTS_SESSION_KEY = "test_analysis_home_reports"
HOME_SELECTED_PATH_SESSION_KEY = "test_analysis_home_path"

STATION_ORDER: List[str] = ["è€¦åˆæµ‹è¯•", "Preæµ‹è¯•", "ä½æ¸©å‚¨å­˜åæµ‹è¯•", "Postæµ‹è¯•", "å°ç›–æµ‹è¯•"]

SUMMARY_COLUMNS: List[str] = ["æœ€å¤§æ•ˆç‡", "åŠŸç‡", "ç”µå‹", "æœ€å¤§ç”µæµ", "çƒ­é˜»"]
NUMERIC_CANDIDATES: List[str] = SUMMARY_COLUMNS + [
    "å³°å€¼æ³¢é•¿",
    "ä¸­å¿ƒæ³¢é•¿",
    "å…‰è°±å…¨é«˜å®½",
    "NA",
]

TEST_TYPE_NORMALIZATION = {
    "è€¦åˆæµ‹è¯•": "è€¦åˆæµ‹è¯•",
    "è€¦åˆ": "è€¦åˆæµ‹è¯•",
    "preæµ‹è¯•": "Preæµ‹è¯•",
    "pretest": "Preæµ‹è¯•",
    "pre": "Preæµ‹è¯•",
    "postæµ‹è¯•": "Postæµ‹è¯•",
    "posttest": "Postæµ‹è¯•",
    "post": "Postæµ‹è¯•",
    "å°ç›–æµ‹è¯•": "å°ç›–æµ‹è¯•",
    "å°ç›–": "å°ç›–æµ‹è¯•",
    "é¡¶ç›–æµ‹è¯•": "å°ç›–æµ‹è¯•",
    "é¡¶ç›–": "å°ç›–æµ‹è¯•",
    "ä½æ¸©å‚¨å­˜åæµ‹è¯•": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "ä½æ¸©å­˜å‚¨åæµ‹è¯•": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "ä½æ¸©åæµ‹è¯•": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "ä½æ¸©å‚¨å­˜åè©¦é©—": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "ä½æ¸©å‚¨å­˜åè¯•éªŒ": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "complete": "å·²å®Œæˆ",
    "å·²å®Œæˆ": "å·²å®Œæˆ",
    "å®Œæˆ": "å·²å®Œæˆ",
}

# å­—ç¬¦æ ‡å‡†åŒ–æ˜ å°„è¡¨ï¼ˆç”¨äºç»Ÿä¸€å…¨è§’/åŠè§’å­—ç¬¦ç­‰ï¼‰
CHAR_NORMALIZATION = str.maketrans({
    "ï¼ˆ": "(",
    "ï¼‰": ")",
    "ï¼…": "%",
    "ï¼š": ":",
    "ï¼Œ": ",",
    "ã€‚": ".",
    "ã€€": " ",  # å…¨è§’ç©ºæ ¼è½¬åŠè§’
})


def is_supported_report(filename: str) -> bool:
    sanitized = filename.strip()
    lower_name = sanitized.lower()
    if not lower_name.endswith(ALLOWED_EXTENSIONS):
        st.error("ä»…æ”¯æŒæ‰©å±•åä¸º .xlsx æˆ– .xls çš„ Excel æŠ¥è¡¨")
        return False
    if not sanitized.startswith(REPORT_PREFIX):
        st.error(f"ä»…æ”¯æŒæ–‡ä»¶åä»¥â€œ{REPORT_PREFIX}â€å¼€å¤´çš„ Excel æŠ¥è¡¨")
        return False
    return True


def load_report_from_path(file_path: str) -> Optional[pd.DataFrame]:
    path = Path(file_path)
    if not path.exists():
        st.error(f"é€‰æ‹©çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        return None
    if not path.is_file():
        st.error(f"é€‰æ‹©çš„è·¯å¾„ä¸æ˜¯æ–‡ä»¶ï¼š{file_path}")
        return None
    if not is_supported_report(path.name):
        return None
    try:
        df = pd.read_excel(path)
    except Exception as exc:  # pragma: no cover - surface to UI
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥ï¼š{exc}")
        return None
    if df.empty:
        st.warning("é€‰æ‹©çš„æŠ¥è¡¨æ²¡æœ‰æ•°æ®ï¼Œè¯·æ£€æŸ¥å†…å®¹åé‡è¯•ã€‚")
        return None
    return df


def normalize_text(value: object) -> str:
    text = "" if value is None else str(value)
    return text.translate(CHAR_NORMALIZATION).strip()


def normalize_test_type(value: object) -> Optional[str]:
    cleaned = normalize_text(value)
    if not cleaned:
        return None
    compact = cleaned.replace(" ", "").lower()
    return TEST_TYPE_NORMALIZATION.get(compact, cleaned)


def parse_uploaded_file(uploaded_file) -> Optional[pd.DataFrame]:
    if uploaded_file is None:
        return None

    try:
        if not is_supported_report(uploaded_file.name):
            return None
        df = pd.read_excel(uploaded_file)
    except Exception as exc:  # pragma: no cover - surface to UI
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥ï¼š{exc}")
        return None

    if df.empty:
        st.warning("ä¸Šä¼ çš„æ–‡ä»¶æ²¡æœ‰æ•°æ®ï¼Œè¯·æ£€æŸ¥å†…å®¹åé‡è¯•ã€‚")
        return None

    return df


def prepare_dataframe(raw: pd.DataFrame) -> Optional[pd.DataFrame]:
    df = raw.copy()
    df.rename(columns={col: normalize_text(col) for col in df.columns}, inplace=True)

    if "æµ‹è¯•ç±»å‹" not in df.columns or "å£³ä½“å·" not in df.columns:
        st.error("æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—ï¼šéœ€è¦åŒ…å«â€œå£³ä½“å·â€å’Œâ€œæµ‹è¯•ç±»å‹â€ã€‚")
        return None

    df["åŸå§‹æµ‹è¯•ç±»å‹"] = df["æµ‹è¯•ç±»å‹"]
    df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"] = df["æµ‹è¯•ç±»å‹"].apply(normalize_test_type)
    df = df[df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"].isin(STATION_ORDER)].copy()

    if df.empty:
        st.warning("æ•°æ®ä¸­æœªæ‰¾åˆ°ç›®æ ‡çš„ 5 ä¸ªæµ‹è¯•ç«™åˆ«ï¼Œè¯·ç¡®è®¤æ–‡ä»¶å†…å®¹ã€‚")
        return None

    df["å£³ä½“å·"] = df["å£³ä½“å·"].astype(str).str.strip()

    for optional in ["è§„æ ¼ç±»å‹", "ç”Ÿäº§è®¢å•", "æ“ä½œäºº"]:
        if optional in df.columns:
            df[optional] = df[optional].astype(str).str.strip()

    if "æµ‹è¯•æ—¶é—´" in df.columns:
        df["æµ‹è¯•æ—¶é—´"] = pd.to_datetime(df["æµ‹è¯•æ—¶é—´"], errors="coerce")
        df["æµ‹è¯•æ—¥æœŸ"] = df["æµ‹è¯•æ—¶é—´"].dt.date
    else:
        df["æµ‹è¯•æ—¥æœŸ"] = pd.NaT

    for column in NUMERIC_CANDIDATES:
        if column in df.columns:
            df[column] = pd.to_numeric(df[column], errors="coerce")

    numeric_cols = [col for col in SUMMARY_COLUMNS if col in df.columns]
    sort_columns: List[str] = ["æ ‡å‡†æµ‹è¯•ç«™åˆ«"]
    if "æµ‹è¯•æ—¶é—´" in df.columns:
        sort_columns.append("æµ‹è¯•æ—¶é—´")
    sort_columns.extend(numeric_cols)
    existing_sort_columns = [col for col in sort_columns if col in df.columns]
    if existing_sort_columns:
        df.sort_values(existing_sort_columns, inplace=True)

    return df.reset_index(drop=True)


def render_station_tab(station: str, station_df: pd.DataFrame) -> None:
    if station_df.empty:
        st.info(f"æš‚æ—  {station} çš„æ•°æ®ã€‚")
        return

    st.subheader(f"{station} æ•°æ®æ¦‚è§ˆ")

    col_a, col_b, col_c = st.columns(3)
    with col_a:
        st.metric("è®°å½•æ•°", len(station_df))
    with col_b:
        if "æœ€å¤§æ•ˆç‡" in station_df.columns and station_df["æœ€å¤§æ•ˆç‡"].notna().any():
            st.metric("å¹³å‡æœ€å¤§æ•ˆç‡", f"{station_df['æœ€å¤§æ•ˆç‡'].mean():.3f}")
        else:
            st.metric("å¹³å‡æœ€å¤§æ•ˆç‡", "â€”")
    with col_c:
        if "çƒ­é˜»" in station_df.columns and station_df["çƒ­é˜»"].notna().any():
            st.metric("å¹³å‡çƒ­é˜»", f"{station_df['çƒ­é˜»'].mean():.3f}")
        else:
            st.metric("å¹³å‡çƒ­é˜»", "â€”")

    stats_columns = [col for col in SUMMARY_COLUMNS if col in station_df.columns]
    if stats_columns:
        summary = station_df[stats_columns].agg(["count", "mean", "std", "min", "max"]).T
        summary.rename(
            columns={"count": "æ•°é‡", "mean": "å¹³å‡å€¼", "std": "æ ‡å‡†å·®", "min": "æœ€å°å€¼", "max": "æœ€å¤§å€¼"},
            inplace=True,
        )
        formatter_map = {col: "{:.3f}" for col in summary.columns if col != "æ•°é‡"}
        st.dataframe(summary.style.format(formatter_map), width='stretch')

    base_columns = ["å£³ä½“å·"]
    for optional in ["è§„æ ¼ç±»å‹", "ç”Ÿäº§è®¢å•", "æµ‹è¯•æ—¶é—´"]:
        if optional in station_df.columns:
            base_columns.append(optional)
    metric_columns = [col for col in NUMERIC_CANDIDATES if col in station_df.columns]
    display_cols = base_columns + metric_columns
    deduped = station_df[display_cols].copy()
    if "æµ‹è¯•æ—¶é—´" in deduped.columns:
        deduped = deduped.sort_values("æµ‹è¯•æ—¶é—´")
    else:
        deduped = deduped.sort_values("å£³ä½“å·")
    st.dataframe(deduped.reset_index(drop=True), width='stretch', height=360)


def render_overview_table(filtered: pd.DataFrame) -> None:
    rows = []
    for station in STATION_ORDER:
        sub = filtered[filtered["æ ‡å‡†æµ‹è¯•ç«™åˆ«"] == station]
        row = {"æµ‹è¯•ç«™åˆ«": station, "è®°å½•æ•°": len(sub)}
        for metric in SUMMARY_COLUMNS:
            if metric in sub.columns and sub[metric].notna().any():
                row[f"{metric}å‡å€¼"] = sub[metric].mean()
        # Add NA metric
        if "NA" in sub.columns and sub["NA"].notna().any():
            row["NAå‡å€¼"] = sub["NA"].mean()
        rows.append(row)

    overview = pd.DataFrame(rows)
    # Transpose the table
    overview_transposed = overview.set_index("æµ‹è¯•ç«™åˆ«").T
    st.dataframe(
        overview_transposed.style.format(
            {
                col: "{:.3f}"
                for col in overview_transposed.columns
            },
            subset=pd.IndexSlice[overview_transposed.index != "è®°å½•æ•°", :],
        ),
        width='stretch',
    )


alt.data_transformers.disable_max_rows()
st.set_page_config(page_title="å¸¸ç”¨æµ‹è¯•æ•°æ®åˆ†æ", page_icon="ğŸ“ˆ", layout="wide")
inject_structured_clone_polyfill()

st.title("ğŸ“ˆ å¸¸ç”¨æµ‹è¯•æ•°æ®åˆ†æ")
st.markdown("ä¸Šä¼ å¸¸ç”¨æµ‹è¯•æ•°æ®æŠ¥è¡¨ï¼ŒæŸ¥çœ‹äº”ä¸ªæµ‹è¯•ç«™åˆ«çš„æŒ‡æ ‡è¡¨ç°ã€‚")

if "test_analysis_df" not in st.session_state:
    st.session_state.test_analysis_df = None
    st.session_state.test_analysis_filename = None
if HOME_SELECTED_PATH_SESSION_KEY not in st.session_state:
    st.session_state[HOME_SELECTED_PATH_SESSION_KEY] = None

uploaded = st.file_uploader(
    "ä¸Šä¼ æµ‹è¯•æ•°æ®ï¼ˆå»ºè®®ä½¿ç”¨å¸¸ç”¨æµ‹è¯•æ•°æ®æŠ¥è¡¨æ ¼å¼ï¼‰",
    type=["xlsx", "xls"],
    help=f"ä»…æ”¯æŒæ–‡ä»¶åä»¥â€œ{REPORT_PREFIX}â€å¼€å¤´çš„ Excel æŠ¥è¡¨ã€‚",
)

if uploaded is not None and uploaded.name != st.session_state.test_analysis_filename:
    with st.spinner("æ­£åœ¨è§£æå¹¶åŠ è½½æ•°æ®..."):
        raw_df = parse_uploaded_file(uploaded)
        if raw_df is not None:
            prepared = prepare_dataframe(raw_df)
            if prepared is not None:
                st.session_state.test_analysis_df = prepared
                st.session_state.test_analysis_filename = uploaded.name
                st.session_state[HOME_SELECTED_PATH_SESSION_KEY] = None
                st.success(f"æ–‡ä»¶ {uploaded.name} è§£ææˆåŠŸï¼Œå…± {len(prepared)} æ¡è®°å½•ã€‚")

home_reports_raw = st.session_state.get(HOME_REPORTS_SESSION_KEY) or []
home_options_map: dict[str, str] = {}
selected_home_path: Optional[str] = None
reload_home_file = False

if home_reports_raw:
    st.markdown("#### æˆ–ä»ä¸»é¡µæ‰«æçš„æŠ¥è¡¨ä¸­é€‰æ‹©")
    home_options = []
    for entry in home_reports_raw:
        if isinstance(entry, dict):
            candidate_path = entry.get("path")
            display_name = entry.get("name") or ""
        else:
            candidate_path = entry
            display_name = ""
        if not candidate_path:
            continue
        if not display_name:
            display_name = Path(candidate_path).name
        base_label = f"{display_name} | {candidate_path}"
        label = base_label
        suffix = 2
        while label in home_options_map:
            label = f"{base_label} ({suffix})"
            suffix += 1
        home_options_map[label] = candidate_path
        home_options.append(label)

    if home_options:
        select_col, refresh_col, reload_col = st.columns([5, 1, 1])
        with select_col:
            selected_label = st.selectbox(
                "ä¸»é¡µè¯†åˆ«çš„ Excel æŠ¥è¡¨",
                options=home_options,
                key="test_analysis_home_file_select",
            )
        with refresh_col:
            st.markdown("<div style='margin-top: 32px;'></div>", unsafe_allow_html=True)
            refresh_home_reports = st.button(
                "ğŸ”„ åˆ·æ–°",
                width='stretch',
                key="test_analysis_refresh_home_reports",
            )
        with reload_col:
            st.markdown("<div style='margin-top: 32px;'></div>", unsafe_allow_html=True)
            reload_home_file = st.button(
                "é‡æ–°åŠ è½½",
                width='stretch',
                key="test_analysis_reload_home_report",
            )
        if refresh_home_reports:
            st.rerun()
        selected_home_path = home_options_map[selected_label]
        st.caption("æç¤ºï¼šåˆ—è¡¨æ¥æºäºä¸»é¡µçš„æ•°æ®æ–‡ä»¶æµè§ˆåŠŸèƒ½ã€‚")
    else:
        st.caption(f"æœªåœ¨ä¸»é¡µæ‰¾åˆ°ä»¥â€œ{REPORT_PREFIX}â€å¼€å¤´çš„ Excel æŠ¥è¡¨ã€‚")

if selected_home_path:
    last_loaded_path = st.session_state.get(HOME_SELECTED_PATH_SESSION_KEY)
    if reload_home_file:
        last_loaded_path = None
    if last_loaded_path != selected_home_path:
        display_name = Path(selected_home_path).name
        with st.spinner(f"æ­£åœ¨åŠ è½½ {display_name}..."):
            home_raw_df = load_report_from_path(selected_home_path)
            if home_raw_df is not None:
                prepared_home = prepare_dataframe(home_raw_df)
                if prepared_home is not None:
                    st.session_state.test_analysis_df = prepared_home
                    st.session_state.test_analysis_filename = display_name
                    st.session_state[HOME_SELECTED_PATH_SESSION_KEY] = selected_home_path
                    st.success(f"æ–‡ä»¶ {display_name} åŠ è½½æˆåŠŸï¼Œå…± {len(prepared_home)} æ¡è®°å½•ã€‚")

dataframe = st.session_state.test_analysis_df
if dataframe is None:
    st.info("è¯·å…ˆä¸Šä¼ æµ‹è¯•æ•°æ®æŠ¥è¡¨ã€‚")
    st.stop()

filtered_df = dataframe.copy()

filters_row = st.columns(4)

with filters_row[0]:
    part_options = sorted(filtered_df["è§„æ ¼ç±»å‹"].dropna().unique()) if "è§„æ ¼ç±»å‹" in filtered_df.columns else []
    selected_parts = st.multiselect("è§„æ ¼ç±»å‹", part_options, default=part_options)
    if selected_parts:
        filtered_df = filtered_df[filtered_df["è§„æ ¼ç±»å‹"].isin(selected_parts)]

with filters_row[1]:
    order_options = sorted(filtered_df["ç”Ÿäº§è®¢å•"].dropna().unique()) if "ç”Ÿäº§è®¢å•" in filtered_df.columns else []
    selected_orders = st.multiselect("ç”Ÿäº§è®¢å•", order_options, default=order_options)
    if selected_orders:
        filtered_df = filtered_df[filtered_df["ç”Ÿäº§è®¢å•"].isin(selected_orders)]

with filters_row[2]:
    station_options = STATION_ORDER
    selected_stations = st.multiselect("æµ‹è¯•ç«™åˆ«", station_options, default=station_options)
    if selected_stations:
        filtered_df = filtered_df[filtered_df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"].isin(selected_stations)]
    else:
        filtered_df = filtered_df.iloc[0:0]

with filters_row[3]:
    if "æµ‹è¯•æ—¶é—´" in filtered_df.columns and filtered_df["æµ‹è¯•æ—¶é—´"].notna().any():
        min_date = filtered_df["æµ‹è¯•æ—¶é—´"].min().date()
        max_date = filtered_df["æµ‹è¯•æ—¶é—´"].max().date()
        start, end = st.date_input(
            "æµ‹è¯•æ—¥æœŸåŒºé—´",
            value=(min_date, max_date),
            min_value=min_date,
            max_value=max_date,
        )
        if start and end:
            mask = filtered_df["æµ‹è¯•æ—¥æœŸ"].between(start, end)
            filtered_df = filtered_df[mask]
    else:
        st.write("æµ‹è¯•æ—¶é—´ç¼ºå¤±")

if filtered_df.empty:
    st.warning("ç­›é€‰æ¡ä»¶ä¸‹æ²¡æœ‰æ•°æ®ï¼Œè¯·è°ƒæ•´è¿‡æ»¤å™¨ã€‚")
    st.stop()

col_left, col_mid, col_right = st.columns(3)
with col_left:
    st.metric("ç­›é€‰åè®°å½•æ•°", len(filtered_df))
with col_mid:
    unique_shells = filtered_df["å£³ä½“å·"].nunique()
    st.metric("å£³ä½“æ•°é‡", unique_shells)
with col_right:
    if "æµ‹è¯•æ—¶é—´" in filtered_df.columns and filtered_df["æµ‹è¯•æ—¶é—´"].notna().any():
        latest_time = filtered_df["æµ‹è¯•æ—¶é—´"].max()
        st.metric("æœ€æ–°æµ‹è¯•æ—¶é—´", latest_time.strftime("%Y-%m-%d %H:%M"))

st.markdown("### ç«™åˆ«æ¦‚è§ˆ")
render_overview_table(filtered_df)

csv_bytes = filtered_df.to_csv(index=False).encode("utf-8-sig")
st.download_button(
    "ğŸ“¥ ä¸‹è½½ç­›é€‰åçš„æ•°æ®ï¼ˆCSVï¼‰",
    data=csv_bytes,
    file_name=f"æµ‹è¯•æ•°æ®ç­›é€‰_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
    mime="text/csv",
)

station_tabs = st.tabs(STATION_ORDER)
for tab, station in zip(station_tabs, STATION_ORDER):
    with tab:
        station_data = filtered_df[filtered_df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"] == station]
        render_station_tab(station, station_data)
