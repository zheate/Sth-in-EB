# title: æµ‹è¯•æ•°æ®åˆ†æ

from datetime import datetime
from pathlib import Path
from typing import List, Optional
import hashlib
import io
import re
import sys

import altair as alt
import pandas as pd
import streamlit as st
from utils.compat import inject_structured_clone_polyfill

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥config
sys.path.insert(0, str(Path(__file__).parent.parent))
from config import DEFAULT_DATA_FOLDER

APP_ROOT = Path(__file__).resolve().parent.parent


def resolve_input_path(path_str: str) -> Path:
    """Resolve user-provided folder path, supporting relative inputs like ./data."""
    normalized = path_str.strip()
    if not normalized:
        raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©º")

    candidate = Path(normalized).expanduser()
    if not candidate.is_absolute():
        candidate = APP_ROOT / candidate
    return candidate.resolve()

REPORT_PREFIX = "å¸¸ç”¨æµ‹è¯•æ•°æ®æŠ¥è¡¨"
ALLOWED_EXTENSIONS = (".xlsx", ".xls")
HOME_REPORTS_SESSION_KEY = "test_analysis_home_reports"
HOME_SELECTED_PATH_SESSION_KEY = "test_analysis_home_path"

STATION_ORDER: List[str] = ["è€¦åˆæµ‹è¯•", "Preæµ‹è¯•", "ä½æ¸©å‚¨å­˜åæµ‹è¯•", "Postæµ‹è¯•", "å°ç›–æµ‹è¯•"]

SUMMARY_COLUMNS: List[str] = ["æœ€å¤§æ•ˆç‡", "åŠŸç‡", "ç”µå‹", "æœ€å¤§ç”µæµ", "çƒ­é˜»"]
NUMERIC_CANDIDATES: List[str] = SUMMARY_COLUMNS + [
    "å³°å€¼æ³¢é•¿",
    "ä¸­å¿ƒæ³¢é•¿",
    "å…‰è°±å…¨é«˜å®½",
    "NA",
]

MAX_AUTOMATIC_SELECTION = 80
MAX_MULTISELECT_OPTIONS = 200

TEST_TYPE_NORMALIZATION = {
    "è€¦åˆæµ‹è¯•": "è€¦åˆæµ‹è¯•",
    "è€¦åˆ": "è€¦åˆæµ‹è¯•",
    "preæµ‹è¯•": "Preæµ‹è¯•",
    "pretest": "Preæµ‹è¯•",
    "pre": "Preæµ‹è¯•",
    "postæµ‹è¯•": "Postæµ‹è¯•",
    "posttest": "Postæµ‹è¯•",
    "post": "Postæµ‹è¯•",
    "å°ç›–æµ‹è¯•": "å°ç›–æµ‹è¯•",
    "å°ç›–": "å°ç›–æµ‹è¯•",
    "é¡¶ç›–æµ‹è¯•": "å°ç›–æµ‹è¯•",
    "é¡¶ç›–": "å°ç›–æµ‹è¯•",
    "ä½æ¸©å‚¨å­˜åæµ‹è¯•": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "ä½æ¸©å­˜å‚¨åæµ‹è¯•": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "ä½æ¸©åæµ‹è¯•": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "ä½æ¸©å‚¨å­˜åè©¦é©—": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "ä½æ¸©å‚¨å­˜åè¯•éªŒ": "ä½æ¸©å‚¨å­˜åæµ‹è¯•",
    "complete": "å·²å®Œæˆ",
    "å·²å®Œæˆ": "å·²å®Œæˆ",
    "å®Œæˆ": "å·²å®Œæˆ",
}

# å­—ç¬¦æ ‡å‡†åŒ–æ˜ å°„è¡¨ï¼ˆç”¨äºç»Ÿä¸€å…¨è§’/åŠè§’å­—ç¬¦ç­‰ï¼‰
CHAR_NORMALIZATION = str.maketrans({
    "ï¼ˆ": "(",
    "ï¼‰": ")",
    "ï¼…": "%",
    "ï¼š": ":",
    "ï¼Œ": ",",
    "ã€‚": ".",
    "ã€€": " ",  # å…¨è§’ç©ºæ ¼è½¬åŠè§’
})


def is_supported_report(filename: str) -> bool:
    sanitized = filename.strip()
    lower_name = sanitized.lower()
    if not lower_name.endswith(ALLOWED_EXTENSIONS):
        st.error("ä»…æ”¯æŒæ‰©å±•åä¸º .xlsx æˆ– .xls çš„ Excel æŠ¥è¡¨")
        return False
    if not sanitized.startswith(REPORT_PREFIX):
        st.error(f"ä»…æ”¯æŒæ–‡ä»¶åä»¥â€œ{REPORT_PREFIX}â€å¼€å¤´çš„ Excel æŠ¥è¡¨")
        return False
    return True


@st.cache_data(show_spinner=False)
def _read_report_from_disk(path_str: str, mtime: float) -> pd.DataFrame:
    return pd.read_excel(path_str)


@st.cache_data(show_spinner=False)
def _read_report_from_bytes(data: bytes, checksum: str) -> pd.DataFrame:
    return pd.read_excel(io.BytesIO(data))


def _invalidate_filter_cache() -> None:
    st.session_state.pop("test_analysis_filter_cache", None)
    st.session_state.pop("test_analysis_filter_cache_version", None)


def _ensure_filter_cache(df: pd.DataFrame) -> dict[str, object]:
    dataset_version = st.session_state.get("test_analysis_dataset_version", 0)
    cache_version = st.session_state.get("test_analysis_filter_cache_version")
    cache = st.session_state.get("test_analysis_filter_cache")
    if cache is None or cache_version != dataset_version:
        part_options: List[str] = []
        if "è§„æ ¼ç±»å‹" in df.columns:
            part_options = sorted(
                df["è§„æ ¼ç±»å‹"].dropna().astype(str).unique().tolist()
            )

        order_options: List[str] = []
        if "ç”Ÿäº§è®¢å•" in df.columns:
            order_options = sorted(
                df["ç”Ÿäº§è®¢å•"].dropna().astype(str).unique().tolist()
            )

        date_range = None
        if "æµ‹è¯•æ—¶é—´" in df.columns:
            valid_times = df["æµ‹è¯•æ—¶é—´"].dropna()
            if not valid_times.empty:
                date_range = (valid_times.min().date(), valid_times.max().date())

        cache = {
            "part_options": part_options,
            "order_options": order_options,
            "date_range": date_range,
        }
        st.session_state.test_analysis_filter_cache = cache
        st.session_state.test_analysis_filter_cache_version = dataset_version
    return cache


def load_report_from_path(file_path: str) -> Optional[pd.DataFrame]:
    path = Path(file_path)
    if not path.exists():
        st.error(f"é€‰æ‹©çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼š{file_path}")
        return None
    if not path.is_file():
        st.error(f"é€‰æ‹©çš„è·¯å¾„ä¸æ˜¯æ–‡ä»¶ï¼š{file_path}")
        return None
    if not is_supported_report(path.name):
        return None
    try:
        resolved = path.resolve()
    except OSError:
        resolved = path
    try:
        mtime = path.stat().st_mtime
    except OSError as exc:
        st.error(f"æ— æ³•è·å–æ–‡ä»¶ä¿¡æ¯ï¼š{exc}")
        return None
    try:
        df = _read_report_from_disk(str(resolved), mtime)
    except Exception as exc:  # pragma: no cover - surface to UI
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥ï¼š{exc}")
        return None
    if df.empty:
        st.warning("é€‰æ‹©çš„æŠ¥è¡¨æ²¡æœ‰æ•°æ®ï¼Œè¯·æ£€æŸ¥å†…å®¹åé‡è¯•ã€‚")
        return None
    return df


def normalize_text(value: object) -> str:
    text = "" if value is None else str(value)
    return text.translate(CHAR_NORMALIZATION).strip()


def normalize_test_type(value: object) -> Optional[str]:
    cleaned = normalize_text(value)
    if not cleaned:
        return None
    compact = cleaned.replace(" ", "").lower()
    return TEST_TYPE_NORMALIZATION.get(compact, cleaned)


def parse_uploaded_file(uploaded_file) -> Optional[pd.DataFrame]:
    if uploaded_file is None:
        return None

    try:
        if not is_supported_report(uploaded_file.name):
            return None
        file_bytes = uploaded_file.getvalue()
        checksum = hashlib.md5(file_bytes).hexdigest()
        df = _read_report_from_bytes(file_bytes, checksum)
    except Exception as exc:  # pragma: no cover - surface to UI
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥ï¼š{exc}")
        return None

    if df.empty:
        st.warning("ä¸Šä¼ çš„æ–‡ä»¶æ²¡æœ‰æ•°æ®ï¼Œè¯·æ£€æŸ¥å†…å®¹åé‡è¯•ã€‚")
        return None

    return df


def prepare_dataframe(raw: pd.DataFrame) -> Optional[pd.DataFrame]:
    df = raw.copy()
    df.rename(columns={col: normalize_text(col) for col in df.columns}, inplace=True)

    if "æµ‹è¯•ç±»å‹" not in df.columns or "å£³ä½“å·" not in df.columns:
        st.error("æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„åˆ—ï¼šéœ€è¦åŒ…å«â€œå£³ä½“å·â€å’Œâ€œæµ‹è¯•ç±»å‹â€ã€‚")
        return None

    df["åŸå§‹æµ‹è¯•ç±»å‹"] = df["æµ‹è¯•ç±»å‹"]
    df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"] = df["æµ‹è¯•ç±»å‹"].apply(normalize_test_type)
    df = df[df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"].isin(STATION_ORDER)].copy()

    if df.empty:
        st.warning("æ•°æ®ä¸­æœªæ‰¾åˆ°ç›®æ ‡çš„ 5 ä¸ªæµ‹è¯•ç«™åˆ«ï¼Œè¯·ç¡®è®¤æ–‡ä»¶å†…å®¹ã€‚")
        return None

    df["å£³ä½“å·"] = df["å£³ä½“å·"].astype(str).str.strip()

    for optional in ["è§„æ ¼ç±»å‹", "ç”Ÿäº§è®¢å•", "æ“ä½œäºº"]:
        if optional in df.columns:
            df[optional] = df[optional].astype(str).str.strip()

    if "æµ‹è¯•æ—¶é—´" in df.columns:
        df["æµ‹è¯•æ—¶é—´"] = pd.to_datetime(df["æµ‹è¯•æ—¶é—´"], errors="coerce")
        df["æµ‹è¯•æ—¥æœŸ"] = df["æµ‹è¯•æ—¶é—´"].dt.date
    else:
        df["æµ‹è¯•æ—¥æœŸ"] = pd.NaT

    for column in NUMERIC_CANDIDATES:
        if column in df.columns:
            df[column] = pd.to_numeric(df[column], errors="coerce")

    numeric_cols = [col for col in SUMMARY_COLUMNS if col in df.columns]
    sort_columns: List[str] = ["æ ‡å‡†æµ‹è¯•ç«™åˆ«"]
    if "æµ‹è¯•æ—¶é—´" in df.columns:
        sort_columns.append("æµ‹è¯•æ—¶é—´")
    sort_columns.extend(numeric_cols)
    existing_sort_columns = [col for col in sort_columns if col in df.columns]
    if existing_sort_columns:
        df.sort_values(existing_sort_columns, inplace=True)

    return df.reset_index(drop=True)


def render_station_tab(station: str, station_df: pd.DataFrame) -> None:
    if station_df.empty:
        st.info(f"æš‚æ—  {station} çš„æ•°æ®ã€‚")
        return

    st.subheader(f"{station} æ•°æ®æ¦‚è§ˆ")

    col_a, col_b, col_c = st.columns(3)
    with col_a:
        st.metric("è®°å½•æ•°", len(station_df))
    with col_b:
        if "æœ€å¤§æ•ˆç‡" in station_df.columns and station_df["æœ€å¤§æ•ˆç‡"].notna().any():
            st.metric("å¹³å‡æœ€å¤§æ•ˆç‡", f"{station_df['æœ€å¤§æ•ˆç‡'].mean():.3f}")
        else:
            st.metric("å¹³å‡æœ€å¤§æ•ˆç‡", "â€”")
    with col_c:
        if "çƒ­é˜»" in station_df.columns and station_df["çƒ­é˜»"].notna().any():
            st.metric("å¹³å‡çƒ­é˜»", f"{station_df['çƒ­é˜»'].mean():.3f}")
        else:
            st.metric("å¹³å‡çƒ­é˜»", "â€”")

    stats_columns = [col for col in SUMMARY_COLUMNS if col in station_df.columns]
    if stats_columns:
        summary = station_df[stats_columns].agg(["count", "mean", "std", "min", "max"]).T
        summary.rename(
            columns={"count": "æ•°é‡", "mean": "å¹³å‡å€¼", "std": "æ ‡å‡†å·®", "min": "æœ€å°å€¼", "max": "æœ€å¤§å€¼"},
            inplace=True,
        )
        summary_display = summary.copy()
        numeric_cols = [col for col in summary_display.columns if col != "æ•°é‡"]
        if numeric_cols:
            summary_display[numeric_cols] = summary_display[numeric_cols].apply(lambda col: col.round(3))
        summary_display["æ•°é‡"] = summary_display["æ•°é‡"].astype(int)
        st.dataframe(
            summary_display.reset_index().rename(columns={"index": "æŒ‡æ ‡"}),
            width='stretch',
            hide_index=True,
        )

    base_columns = ["å£³ä½“å·"]
    for optional in ["è§„æ ¼ç±»å‹", "ç”Ÿäº§è®¢å•", "æµ‹è¯•æ—¶é—´"]:
        if optional in station_df.columns:
            base_columns.append(optional)
    metric_columns = [col for col in NUMERIC_CANDIDATES if col in station_df.columns]
    display_cols = base_columns + metric_columns
    deduped = station_df[display_cols].copy()
    if "æµ‹è¯•æ—¶é—´" in deduped.columns:
        deduped = deduped.sort_values("æµ‹è¯•æ—¶é—´")
    else:
        deduped = deduped.sort_values("å£³ä½“å·")
    with st.expander("æŸ¥çœ‹è¯¦ç»†è®°å½•ï¼ˆæœ€å¤šå‰ 200 è¡Œï¼‰", expanded=False):
        preview = deduped.reset_index(drop=True).head(200)
        st.dataframe(
            preview,
            width='stretch',
            hide_index=True,
            height=min(600, max(200, len(preview) * 35)),
        )
        if len(deduped) > len(preview):
            st.caption(f"ä»…å±•ç¤ºå‰ {len(preview)} è¡Œå®Œæ•´è®°å½•ï¼Œå¯é€šè¿‡ä¸‹è½½åŠŸèƒ½è·å–å…¨éƒ¨æ•°æ®ã€‚")


def render_overview_table(filtered: pd.DataFrame) -> None:
    rows = []
    for station in STATION_ORDER:
        sub = filtered[filtered["æ ‡å‡†æµ‹è¯•ç«™åˆ«"] == station]
        row = {"æµ‹è¯•ç«™åˆ«": station, "è®°å½•æ•°": len(sub)}
        for metric in SUMMARY_COLUMNS:
            if metric in sub.columns and sub[metric].notna().any():
                row[f"{metric}å‡å€¼"] = sub[metric].mean()
        if "NA" in sub.columns and sub["NA"].notna().any():
            row["NAå‡å€¼"] = sub["NA"].mean()
        rows.append(row)

    overview = pd.DataFrame(rows)
    overview_display = overview.set_index("æµ‹è¯•ç«™åˆ«")
    numeric_cols = [col for col in overview_display.columns if col != "è®°å½•æ•°"]
    if numeric_cols:
        overview_display[numeric_cols] = overview_display[numeric_cols].apply(lambda col: col.round(3))
    st.dataframe(
        overview_display.reset_index(),
        width='stretch',
        hide_index=True,
    )


alt.data_transformers.disable_max_rows()
st.set_page_config(page_title="å¸¸ç”¨æµ‹è¯•æ•°æ®åˆ†æ", page_icon="ğŸ“ˆ", layout="wide")
inject_structured_clone_polyfill()

st.title("ğŸ“ˆ å¸¸ç”¨æµ‹è¯•æ•°æ®åˆ†æ")
st.markdown("ä¸Šä¼ å¸¸ç”¨æµ‹è¯•æ•°æ®æŠ¥è¡¨ï¼ŒæŸ¥çœ‹äº”ä¸ªæµ‹è¯•ç«™åˆ«çš„æŒ‡æ ‡è¡¨ç°ã€‚")

if "test_analysis_df" not in st.session_state:
    st.session_state.test_analysis_df = None
    st.session_state.test_analysis_filename = None
if HOME_SELECTED_PATH_SESSION_KEY not in st.session_state:
    st.session_state[HOME_SELECTED_PATH_SESSION_KEY] = None
if "test_analysis_dataset_version" not in st.session_state:
    st.session_state.test_analysis_dataset_version = 0

uploaded = st.file_uploader(
    "ä¸Šä¼ æµ‹è¯•æ•°æ®ï¼ˆå»ºè®®ä½¿ç”¨å¸¸ç”¨æµ‹è¯•æ•°æ®æŠ¥è¡¨æ ¼å¼ï¼‰",
    type=["xlsx", "xls"],
    help=f"ä»…æ”¯æŒæ–‡ä»¶åä»¥â€œ{REPORT_PREFIX}â€å¼€å¤´çš„ Excel æŠ¥è¡¨ã€‚",
)

if uploaded is not None and uploaded.name != st.session_state.test_analysis_filename:
    with st.spinner("æ­£åœ¨è§£æå¹¶åŠ è½½æ•°æ®..."):
        raw_df = parse_uploaded_file(uploaded)
        if raw_df is not None:
            prepared = prepare_dataframe(raw_df)
            if prepared is not None:
                st.session_state.test_analysis_df = prepared
                st.session_state.test_analysis_filename = uploaded.name
                st.session_state[HOME_SELECTED_PATH_SESSION_KEY] = None
                st.session_state.test_analysis_dataset_version = (
                    st.session_state.get("test_analysis_dataset_version", 0) + 1
                )
                _invalidate_filter_cache()
                st.success(f"æ–‡ä»¶ {uploaded.name} è§£ææˆåŠŸï¼Œå…± {len(prepared)} æ¡è®°å½•ã€‚")

home_reports_raw = st.session_state.get(HOME_REPORTS_SESSION_KEY) or []
home_options_map: dict[str, str] = {}
selected_home_path: Optional[str] = None
reload_home_file = False

if home_reports_raw:
    st.markdown("#### æˆ–ä»ä¸»é¡µæ‰«æçš„æŠ¥è¡¨ä¸­é€‰æ‹©")
    home_options = []
    for entry in home_reports_raw:
        if isinstance(entry, dict):
            candidate_path = entry.get("path")
            display_name = entry.get("name") or ""
        else:
            candidate_path = entry
            display_name = ""
        if not candidate_path:
            continue
        if not display_name:
            display_name = Path(candidate_path).name
        base_label = f"{display_name} | {candidate_path}"
        label = base_label
        suffix = 2
        while label in home_options_map:
            label = f"{base_label} ({suffix})"
            suffix += 1
        home_options_map[label] = candidate_path
        home_options.append(label)

    if home_options:
        select_col, refresh_col, reload_col = st.columns([5, 1, 1])
        with select_col:
            selected_label = st.selectbox(
                "ä¸»é¡µè¯†åˆ«çš„ Excel æŠ¥è¡¨",
                options=home_options,
                key="test_analysis_home_file_select",
            )
        with refresh_col:
            st.markdown("<div style='margin-top: 32px;'></div>", unsafe_allow_html=True)
            refresh_home_reports = st.button(
                "ğŸ”„ åˆ·æ–°",
                width='stretch',
                key="test_analysis_refresh_home_reports",
            )
        with reload_col:
            st.markdown("<div style='margin-top: 32px;'></div>", unsafe_allow_html=True)
            reload_home_file = st.button(
                "é‡æ–°åŠ è½½",
                width='stretch',
                key="test_analysis_reload_home_report",
            )
        if refresh_home_reports:
            st.rerun()
        selected_home_path = home_options_map[selected_label]
        st.caption("æç¤ºï¼šåˆ—è¡¨æ¥æºäºä¸»é¡µçš„æ•°æ®æ–‡ä»¶æµè§ˆåŠŸèƒ½ã€‚")
    else:
        st.caption(f"æœªåœ¨ä¸»é¡µæ‰¾åˆ°ä»¥â€œ{REPORT_PREFIX}â€å¼€å¤´çš„ Excel æŠ¥è¡¨ã€‚")

if selected_home_path:
    last_loaded_path = st.session_state.get(HOME_SELECTED_PATH_SESSION_KEY)
    if reload_home_file:
        last_loaded_path = None
    if last_loaded_path != selected_home_path:
        display_name = Path(selected_home_path).name
        with st.spinner(f"æ­£åœ¨åŠ è½½ {display_name}..."):
            home_raw_df = load_report_from_path(selected_home_path)
            if home_raw_df is not None:
                prepared_home = prepare_dataframe(home_raw_df)
                if prepared_home is not None:
                    st.session_state.test_analysis_df = prepared_home
                    st.session_state.test_analysis_filename = display_name
                    st.session_state[HOME_SELECTED_PATH_SESSION_KEY] = selected_home_path
                    st.session_state.test_analysis_dataset_version = (
                        st.session_state.get("test_analysis_dataset_version", 0) + 1
                    )
                    _invalidate_filter_cache()
                    st.success(f"æ–‡ä»¶ {display_name} åŠ è½½æˆåŠŸï¼Œå…± {len(prepared_home)} æ¡è®°å½•ã€‚")

dataframe = st.session_state.test_analysis_df
if dataframe is None:
    st.info("è¯·å…ˆä¸Šä¼ æµ‹è¯•æ•°æ®æŠ¥è¡¨ã€‚")
    st.stop()

filtered_df = dataframe
filter_cache = _ensure_filter_cache(dataframe)

filters_row = st.columns(4)

with filters_row[0]:
    part_options = filter_cache.get("part_options", [])
    if part_options:
        if len(part_options) <= MAX_MULTISELECT_OPTIONS:
            default_parts = part_options if len(part_options) <= MAX_AUTOMATIC_SELECTION else []
            selected_parts = st.multiselect(
                "è§„æ ¼ç±»å‹",
                part_options,
                default=default_parts,
                placeholder="å¯è¾“å…¥å…³é”®å­—è¿‡æ»¤",
                key="test_analysis_part_multiselect",
            )
            if len(part_options) > MAX_AUTOMATIC_SELECTION:
                st.caption(f"æ£€æµ‹åˆ° {len(part_options)} ç§è§„æ ¼ç±»å‹ï¼Œé»˜è®¤ä¸å‹¾é€‰ï¼Œå¯æ‰‹åŠ¨æŒ‘é€‰ã€‚")
            if selected_parts and "è§„æ ¼ç±»å‹" in filtered_df.columns:
                filtered_df = filtered_df[filtered_df["è§„æ ¼ç±»å‹"].isin(selected_parts)]
        else:
            st.caption(f"è§„æ ¼ç±»å‹é¡¹è¾ƒå¤šï¼ˆ{len(part_options)} é¡¹ï¼‰ï¼Œè¯·ä½¿ç”¨å…³é”®è¯ç­›é€‰ã€‚")
            keywords = st.text_input(
                "è§„æ ¼ç±»å‹å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
                key="test_analysis_part_keywords",
            )
            if keywords.strip() and "è§„æ ¼ç±»å‹" in filtered_df.columns:
                terms = [token.strip() for token in keywords.split(',') if token.strip()]
                if terms:
                    series = filtered_df["è§„æ ¼ç±»å‹"].astype(str)
                    mask = pd.Series(False, index=series.index)
                    for term in terms:
                        mask |= series.str.contains(re.escape(term), case=False, na=False)
                    filtered_df = filtered_df[mask]
    else:
        st.write("è§„æ ¼ç±»å‹åˆ—ç¼ºå¤±")

with filters_row[1]:
    order_options = filter_cache.get("order_options", [])
    if order_options:
        if len(order_options) <= MAX_MULTISELECT_OPTIONS:
            default_orders = order_options if len(order_options) <= MAX_AUTOMATIC_SELECTION else []
            selected_orders = st.multiselect(
                "ç”Ÿäº§è®¢å•",
                order_options,
                default=default_orders,
                placeholder="å¯è¾“å…¥è®¢å•ç¼–å·",
                key="test_analysis_order_multiselect",
            )
            if len(order_options) > MAX_AUTOMATIC_SELECTION:
                st.caption(f"æ£€æµ‹åˆ° {len(order_options)} ä¸ªè®¢å•å·ï¼Œé»˜è®¤ä¸å‹¾é€‰ï¼Œå¯æ‰‹åŠ¨æŒ‘é€‰ã€‚")
            if selected_orders and "ç”Ÿäº§è®¢å•" in filtered_df.columns:
                filtered_df = filtered_df[filtered_df["ç”Ÿäº§è®¢å•"].isin(selected_orders)]
        else:
            st.caption(f"è®¢å•å·æ•°é‡è¾ƒå¤šï¼ˆ{len(order_options)} ä¸ªï¼‰ï¼Œè¯·è¾“å…¥å…³é”®è¯ç­›é€‰ã€‚")
            order_keywords = st.text_input(
                "è®¢å•å…³é”®è¯ï¼ˆé€—å·åˆ†éš”ï¼‰",
                key="test_analysis_order_keywords",
            )
            if order_keywords.strip() and "ç”Ÿäº§è®¢å•" in filtered_df.columns:
                terms = [token.strip() for token in order_keywords.split(',') if token.strip()]
                if terms:
                    series = filtered_df["ç”Ÿäº§è®¢å•"].astype(str)
                    mask = pd.Series(False, index=series.index)
                    for term in terms:
                        mask |= series.str.contains(re.escape(term), case=False, na=False)
                    filtered_df = filtered_df[mask]
    else:
        st.write("ç”Ÿäº§è®¢å•åˆ—ç¼ºå¤±")

with filters_row[2]:
    selected_stations = st.multiselect("æµ‹è¯•ç«™åˆ«", STATION_ORDER, default=STATION_ORDER)
    if selected_stations:
        if "æ ‡å‡†æµ‹è¯•ç«™åˆ«" in filtered_df.columns:
            filtered_df = filtered_df[filtered_df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"].isin(selected_stations)]
    else:
        filtered_df = filtered_df.iloc[0:0]

with filters_row[3]:
    date_range = filter_cache.get("date_range")
    if date_range and "æµ‹è¯•æ—¥æœŸ" in filtered_df.columns:
        start, end = st.date_input(
            "æµ‹è¯•æ—¥æœŸåŒºé—´",
            value=date_range,
            min_value=date_range[0],
            max_value=date_range[1],
            key="test_analysis_date_range",
        )
        if start and end:
            mask = filtered_df["æµ‹è¯•æ—¥æœŸ"].between(start, end)
            filtered_df = filtered_df[mask]
    else:
        st.write("æµ‹è¯•æ—¶é—´ç¼ºå¤±")

if filtered_df.empty:
    st.warning("ç­›é€‰æ¡ä»¶ä¸‹æ²¡æœ‰æ•°æ®ï¼Œè¯·è°ƒæ•´è¿‡æ»¤å™¨ã€‚")
    st.stop()


st.caption('ç­›é€‰ç»“æœå·²ç¼“å­˜ï¼Œå¯åœ¨â€œæ•°æ®åˆ†æâ€é¡µç»Ÿä¸€ä¿å­˜ã€‚')

with st.expander("æŸ¥çœ‹ç­›é€‰ç»“æœé¢„è§ˆï¼ˆæœ€å¤šå‰ 200 è¡Œï¼‰", expanded=False):
    preview = filtered_df.head(200)
    st.dataframe(
        preview,
        width='stretch',
        height=min(600, max(200, len(preview) * 35)),
    )
    if len(filtered_df) > len(preview):
        st.caption(f"ä»…å±•ç¤ºå‰ {len(preview)} è¡Œã€‚å®Œæ•´æ•°æ®å¯é€šè¿‡ä¸‹æ–¹ä¸‹è½½æŒ‰é’®è·å–ã€‚")

col_left, col_mid, col_right = st.columns(3)
with col_left:
    st.metric("ç­›é€‰åè®°å½•æ•°", len(filtered_df))
with col_mid:
    unique_shells = filtered_df["å£³ä½“å·"].nunique()
    st.metric("å£³ä½“æ•°é‡", unique_shells)
with col_right:
    if "æµ‹è¯•æ—¶é—´" in filtered_df.columns and filtered_df["æµ‹è¯•æ—¶é—´"].notna().any():
        latest_time = filtered_df["æµ‹è¯•æ—¶é—´"].max()
        st.metric("æœ€æ–°æµ‹è¯•æ—¶é—´", latest_time.strftime("%Y-%m-%d %H:%M"))

st.markdown("### ç«™åˆ«æ¦‚è§ˆ")
render_overview_table(filtered_df)

csv_bytes = filtered_df.to_csv(index=False).encode("utf-8-sig")
st.download_button(
    "ğŸ“¥ ä¸‹è½½ç­›é€‰åçš„æ•°æ®ï¼ˆCSVï¼‰",
    data=csv_bytes,
    file_name=f"æµ‹è¯•æ•°æ®ç­›é€‰_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
    mime="text/csv",
)

if "æ ‡å‡†æµ‹è¯•ç«™åˆ«" not in filtered_df.columns:
    st.warning("æ•°æ®ä¸­ç¼ºå°‘â€œæ ‡å‡†æµ‹è¯•ç«™åˆ«â€å­—æ®µï¼Œæ— æ³•å±•ç¤ºç«™ç‚¹è¯¦æƒ…ã€‚")
else:
    available_stations = [
        station
        for station in STATION_ORDER
        if station in filtered_df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"].unique()
    ]
    if not available_stations:
        st.warning("ç­›é€‰ç»“æœä¸­æ²¡æœ‰ç¬¦åˆçš„ç«™åˆ«æ•°æ®ã€‚")
    else:
        selected_station = st.selectbox("é€‰æ‹©æµ‹è¯•ç«™åˆ«æŸ¥çœ‹è¯¦æƒ…", available_stations, index=0)
        station_data = filtered_df[filtered_df["æ ‡å‡†æµ‹è¯•ç«™åˆ«"] == selected_station]
        render_station_tab(selected_station, station_data)
