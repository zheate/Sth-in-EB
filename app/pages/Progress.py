# title: è¿›åº¦è¿½è¸ª
from pathlib import Path
from typing import List, Optional, Union, Tuple
import sys
import time

import altair as alt
import pandas as pd
import streamlit as st

# Ensure we can import project modules
sys.path.insert(0, str(Path(__file__).parent.parent))

from config import DEFAULT_DATA_FOLDER, WIP_REPORT_KEYWORDS
from pages.data_manager.constants import (
    BASE_STATIONS,
    BASE_STATIONS_LOWER,
    STATION_MAPPING,
    STATION_MAPPING_LOWER,
    get_stations_for_part,
)
from pages.data_manager.product_type_service import ProductTypeService
from utils.local_storage import DataCategory, LocalDataStore
from utils.exceptions import StorageError
from utils.storage_widgets import render_load_selector

APP_ROOT = Path(__file__).resolve().parent.parent
ALLOWED_PATH_ROOTS = [APP_ROOT, Path(DEFAULT_DATA_FOLDER).resolve()]

PRODUCTION_ORDER_CANDIDATES: List[str] = [
    "ç”Ÿäº§è®¢å•",
    "ERPç”Ÿäº§è®¢å•",
    "SAPç”Ÿäº§è®¢å•",
    "ç”Ÿäº§è®¢å•å·",
    "è®¢å•å·",
    "å·¥å•å·",
]


def _pick_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    """Pick a column from a list of candidates (case insensitive)."""
    lower_map = {str(c).lower(): c for c in df.columns}
    for name in candidates:
        if name in df.columns:
            return name
        if name.lower() in lower_map:
            return lower_map[name.lower()]
    return None


def resolve_input_path(path_str: str) -> Path:
    """Resolve and validate a user supplied folder path."""
    normalized = path_str.strip()
    if not normalized:
        raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©º")

    candidate = Path(normalized).expanduser()
    if not candidate.is_absolute():
        candidate = APP_ROOT / candidate
    resolved = candidate.resolve()

    if not resolved.exists():
        raise ValueError(f"è·¯å¾„ä¸å­˜åœ¨: {resolved}")
    if not resolved.is_dir():
        raise ValueError(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶å¤¹: {resolved}")

    is_allowed = any(resolved == allowed or allowed in resolved.parents for allowed in ALLOWED_PATH_ROOTS)
    if not is_allowed:
        raise ValueError(f"è·¯å¾„ä¸åœ¨å…è®¸çš„èŒƒå›´å†…: {resolved}")
    return resolved


def _compute_usecols(header_cols: List[str]) -> List[str]:
    cols_set = {str(c).strip() for c in header_cols}
    needed = {"å£³ä½“å·", "æ–™å·", "å½“å‰ç«™ç‚¹", "ä¸Šä¸€ç«™"}
    for name in PRODUCTION_ORDER_CANDIDATES:
        if name in cols_set:
            needed.add(name)
            break
    for excel_col in STATION_MAPPING.keys():
        time_col = f"{excel_col}æ—¶é—´"
        if time_col in cols_set:
            needed.add(time_col)
    return [c for c in header_cols if str(c).strip() in needed]


def read_data_file(file_path: Union[str, Path], usecols: Optional[List[str]] = None) -> pd.DataFrame:
    """Read CSV/Excel with basic encoding handling."""
    file_path = Path(file_path)
    suffix = file_path.suffix.lower()

    if suffix == ".csv":
        for encoding in ("utf-8", "utf-8-sig", "gbk", "gb18030"):
            try:
                return pd.read_csv(file_path, encoding=encoding, usecols=usecols, low_memory=False)
            except UnicodeDecodeError:
                continue
        raise ValueError(f"æ— æ³•è§£æ CSV æ–‡ä»¶ç¼–ç : {file_path.name}")
    if suffix in (".xls", ".xlsx"):
        engine = "openpyxl" if suffix == ".xlsx" else None
        return pd.read_excel(file_path, usecols=usecols, engine=engine)
    raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {suffix}")


def parse_uploaded_file(uploaded_file) -> Optional[pd.DataFrame]:
    """Parse uploaded CSV/Excel file."""
    try:
        name = uploaded_file.name.lower()
        if name.endswith(".csv"):
            for encoding in ("utf-8", "utf-8-sig", "gbk", "gb18030"):
                try:
                    uploaded_file.seek(0)
                    return pd.read_csv(uploaded_file, encoding=encoding)
                except UnicodeDecodeError:
                    continue
            st.error("æ— æ³•è§£æ CSV æ–‡ä»¶ç¼–ç ")
            return None
        if name.endswith((".xls", ".xlsx")):
            return pd.read_excel(uploaded_file)
        st.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  CSV æˆ– Excel æ–‡ä»¶")
        return None
    except Exception as e:
        st.error(f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")
        return None


def normalize_station_name(station_name) -> str:
    """Normalize station names using the shared mapping."""
    if pd.isna(station_name) or station_name == "":
        return ""

    station_name = str(station_name).strip()
    station_name_lower = station_name.lower()

    if station_name_lower in STATION_MAPPING_LOWER:
        return STATION_MAPPING_LOWER[station_name_lower]

    if station_name.endswith("æµ‹è¯•") or station_name_lower.endswith("æµ‹è¯•"):
        base_name = station_name[:-2]
        base_name_lower = base_name.lower()
        if base_name_lower in STATION_MAPPING_LOWER:
            return STATION_MAPPING_LOWER[base_name_lower]

    test_name_lower = station_name_lower + "æµ‹è¯•"
    if test_name_lower in STATION_MAPPING_LOWER:
        return STATION_MAPPING_LOWER[test_name_lower]

    if station_name_lower in BASE_STATIONS_LOWER:
        return BASE_STATIONS_LOWER[station_name_lower]

    # è¿”å›åŸå§‹åç§°
    return station_name


def extract_progress_data(df: pd.DataFrame, light: bool = False) -> pd.DataFrame:
    """Extract normalized progress information from raw WIP data."""
    if df is None or df.empty:
        return pd.DataFrame()

    shell_col = _pick_column(df, ["å£³ä½“å·", "å£³ä½“ç¼–ç ", "å£³ä½“", "è…”ä½“å·", "è…”ä½“ç¼–å·", "Shell ID", "ShellID", "SN", "åºåˆ—å·"])
    current_col = _pick_column(df, ["å½“å‰ç«™ç‚¹", "å½“å‰ç«™", "æœ€æ–°ç«™ç‚¹", "å½“å‰å·¥åº", "ç«™åˆ«"])
    prev_col = _pick_column(df, ["ä¸Šä¸€ç«™", "ä¸Šä¸€ç«™ç‚¹", "ä¸Šä¸€æ­¥"])
    part_col = _pick_column(df, ["æ–™å·", "äº§å“æ–™å·", "ç‰©æ–™å·", "æ–™å·ç¼–ç "])
    production_order_column = _pick_column(df, PRODUCTION_ORDER_CANDIDATES)

    if shell_col is None:
        st.error("æœªæ‰¾åˆ°å£³ä½“å·åˆ—ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å†…å®¹")
        return pd.DataFrame()

    df = df.copy()
    df[shell_col] = df[shell_col].fillna("").astype(str).str.strip()
    df = df[df[shell_col] != ""].reset_index(drop=True)

    result = pd.DataFrame()
    result["å£³ä½“å·"] = df[shell_col]
    result["æ–™å·"] = df[part_col].fillna("").astype(str).str.strip() if part_col else ""

    if production_order_column and production_order_column in df.columns:
        result["ç”Ÿäº§è®¢å•"] = df[production_order_column].fillna("").astype(str).str.strip()
    else:
        result["ç”Ÿäº§è®¢å•"] = ""

    if current_col and current_col in df.columns:
        result["å½“å‰ç«™ç‚¹åŸå§‹"] = df[current_col].fillna("").astype(str)
    else:
        result["å½“å‰ç«™ç‚¹åŸå§‹"] = ""
    result["å½“å‰ç«™ç‚¹"] = result["å½“å‰ç«™ç‚¹åŸå§‹"].apply(normalize_station_name)
    result["ä¸Šä¸€ç«™"] = df[prev_col].fillna("").astype(str) if prev_col and prev_col in df.columns else ""
    result["æ˜¯å¦å·¥ç¨‹åˆ†æ"] = result["å½“å‰ç«™ç‚¹"] == "å·¥ç¨‹åˆ†æ"

    existing_station_time_cols = [
        (excel_col, STATION_MAPPING[excel_col], f"{excel_col}æ—¶é—´")
        for excel_col in STATION_MAPPING.keys()
        if f"{excel_col}æ—¶é—´" in df.columns
    ]

    def compute_completed_stations(row_idx: int):
        completed = []
        time_map = {}
        for _, standard_station, time_col in existing_station_time_cols:
            val = df.at[row_idx, time_col]
            if pd.notna(val) and str(val).strip():
                completed.append(standard_station)
                time_map[standard_station] = val
        return completed, time_map

    completed_data = [compute_completed_stations(i) for i in df.index]
    result["å®Œæˆç«™åˆ«"] = [c[0] for c in completed_data]
    result["ç«™åˆ«æ—¶é—´"] = [c[1] for c in completed_data]

    unrecognized = result[
        (result["å½“å‰ç«™ç‚¹åŸå§‹"] != "")
        & (result["å½“å‰ç«™ç‚¹"] == result["å½“å‰ç«™ç‚¹åŸå§‹"])
        & (~result["å½“å‰ç«™ç‚¹"].isin(BASE_STATIONS))
        & (~result["å½“å‰ç«™ç‚¹"].isin({"å·¥ç¨‹åˆ†æ", "å·²å®Œæˆ", "æœªå¼€å§‹"}))
    ]["å½“å‰ç«™ç‚¹åŸå§‹"].unique()

    if len(unrecognized) > 0:
        st.warning(f"âš ï¸ å‘ç°æœªè¯†åˆ«çš„ç«™åˆ«åç§°: {', '.join(sorted(unrecognized))}")

    result.attrs["production_order_column"] = production_order_column
    result.attrs["time_cols"] = [tc for _, _, tc in existing_station_time_cols]
    result.attrs["shell_col"] = shell_col
    return result


def calculate_station_counts(progress_df: pd.DataFrame) -> pd.DataFrame:
    """ç»Ÿè®¡å„å½“å‰ç«™åˆ«çš„å£³ä½“æ•°é‡ä¸å æ¯”"""
    if progress_df.empty:
        return pd.DataFrame(columns=["ç«™åˆ«", "æ•°é‡", "å æ¯”"])

    unknown_label = "æœªè¯†åˆ«"
    station_series = progress_df["å½“å‰ç«™ç‚¹"].fillna("").astype(str).str.strip()
    station_series = station_series.replace({"": unknown_label, "nan": unknown_label})
    station_series = station_series.apply(lambda value: normalize_station_name(value) if value != unknown_label else value)

    counts = station_series.value_counts(dropna=False).reset_index()
    counts.columns = ["ç«™åˆ«", "æ•°é‡"]
    counts["å æ¯”"] = counts["æ•°é‡"] / len(progress_df)

    ordered_labels = BASE_STATIONS + ["å·¥ç¨‹åˆ†æ", "å·²å®Œæˆ", unknown_label]
    order_map = {label: idx for idx, label in enumerate(ordered_labels)}
    counts["æ’åº"] = counts["ç«™åˆ«"].map(order_map)

    fallback_order = len(ordered_labels) + counts.index.to_series()
    counts["æ’åº"] = counts["æ’åº"].fillna(fallback_order)
    counts = counts.sort_values(["æ’åº", "ç«™åˆ«"]).drop(columns="æ’åº").reset_index(drop=True)
    return counts


def create_progress_table(progress_df: pd.DataFrame) -> pd.DataFrame:
    """åˆ›å»ºè¿›åº¦è¡¨æ ¼"""
    table_data = []

    for _, row in progress_df.iterrows():
        part_number = row.get("æ–™å·", "")
        stations = get_stations_for_part(part_number)
        current_station = row.get("å½“å‰ç«™ç‚¹", "")
        is_engineering = row.get("æ˜¯å¦å·¥ç¨‹åˆ†æ", False)

        if is_engineering and "å·¥ç¨‹åˆ†æ" not in stations:
            stations.append("å·¥ç¨‹åˆ†æ")

        station_order = -1
        completed_count = 0

        if is_engineering:
            last_station = row.get("ä¸Šä¸€ç«™", "")
            last_station_normalized = normalize_station_name(last_station)
            if last_station_normalized and last_station_normalized in stations:
                station_order = stations.index(last_station_normalized)
                completed_count = station_order + 1
        elif current_station == "å·²å®Œæˆ":
            station_order = len(stations) - 1
            completed_count = len(stations)
        elif current_station and current_station in stations:
            station_order = stations.index(current_station)
            completed_count = station_order
        else:
            completed_count = len(row.get("å®Œæˆç«™åˆ«", []))

        total_count = len(stations)
        progress_pct = (completed_count / total_count * 100) if total_count > 0 else 0

        last_completed_station = ""
        if station_order > 0:
            last_completed_station = stations[station_order - 1]
        elif row.get("å®Œæˆç«™åˆ«"):
            last_completed_station = row["å®Œæˆç«™åˆ«"][-1]

        table_data.append(
            {
                "å£³ä½“å·": row.get("å£³ä½“å·", ""),
                "æ–™å·": part_number,
                "ç”Ÿäº§è®¢å•": row.get("ç”Ÿäº§è®¢å•", ""),
                "å½“å‰ç«™ç‚¹": current_station,
                "å·²å®Œæˆç«™åˆ«æ•°": completed_count,
                "æ€»ç«™åˆ«æ•°": total_count,
                "å®Œæˆè¿›åº¦": f"{progress_pct:.1f}%",
                "æœ€æ–°å®Œæˆç«™åˆ«": last_completed_station,
                "æ˜¯å¦å·¥ç¨‹åˆ†æ": "æ˜¯" if is_engineering else "å¦",
                "ç«™åˆ«åºå·": station_order,
            }
        )

    result_df = pd.DataFrame(table_data)
    if "ç«™åˆ«åºå·" in result_df.columns:
        if not result_df.empty:
            result_df = result_df.sort_values("ç«™åˆ«åºå·", ascending=True)
        result_df = result_df.drop(columns=["ç«™åˆ«åºå·"], errors="ignore")
    return result_df


def get_product_type_service() -> ProductTypeService:
    """Get ProductTypeService instance (refresh if missing new methods)."""
    service = st.session_state.get("progress_product_type_service")
    if service is None or not hasattr(service, "upsert_product_type"):
        service = ProductTypeService()
        st.session_state["progress_product_type_service"] = service
    return service


def prepare_shells_dataframe_for_data_manager(progress_df: pd.DataFrame) -> pd.DataFrame:
    """
    Flatten progress DataFrame to the format expected by Data Manager.
    
    - å±•å¼€"ç«™åˆ«æ—¶é—´"å­—å…¸ä¸ºç‹¬ç«‹çš„ç«™åˆ«æ—¶é—´åˆ—
    - å»é‡å£³ä½“å·
    - ç”Ÿæˆ"æ›´æ–°æ—¶é—´"åˆ—ï¼ˆå„ç«™åˆ«æ—¶é—´çš„æœ€æ™šæ—¶é—´ï¼‰
    """
    if progress_df is None or progress_df.empty:
        raise ValueError("æ²¡æœ‰å¯ä¿å­˜çš„è¿›åº¦æ•°æ®")

    shells_df = progress_df.copy()

    time_cols: List[str] = []
    if "ç«™åˆ«æ—¶é—´" in shells_df.columns:
        time_dicts = shells_df["ç«™åˆ«æ—¶é—´"].apply(lambda v: v if isinstance(v, dict) else {})
        all_stations = sorted({s for d in time_dicts for s in d.keys()})
        for station in all_stations:
            col_name = f"{station}æ—¶é—´"
            shells_df[col_name] = time_dicts.apply(lambda d: d.get(station))
            time_cols.append(col_name)

    # åˆ é™¤å­—å…¸åˆ—ï¼Œä¿ç•™å±•å¼€åçš„åˆ—
    shells_df = shells_df.drop(columns=["ç«™åˆ«æ—¶é—´"], errors="ignore")

    if time_cols:
        shells_df[time_cols] = shells_df[time_cols].apply(pd.to_datetime, errors="coerce")
        shells_df["æ›´æ–°æ—¶é—´"] = shells_df[time_cols].apply(
            lambda row: pd.to_datetime(row.dropna()).max() if row.notna().any() else pd.NaT,
            axis=1,
        )

    shell_col = _pick_column(shells_df, ["å£³ä½“å·", "å£³ä½“ç¼–ç ", "å£³ä½“", "è…”ä½“å·", "è…”ä½“ç¼–å·", "Shell ID", "ShellID", "SN", "åºåˆ—å·"])
    if shell_col:
        shells_df = shells_df.drop_duplicates(subset=[shell_col]).reset_index(drop=True)

    return shells_df


# ============================================================================
# Streamlit é¡µé¢
# ============================================================================

st.set_page_config(page_title="æ¨¡å—è¿›åº¦", page_icon="ğŸ“Š", layout="wide")
st.title("æ¨¡å—WIPè¿›åº¦")

st.markdown(
    """
<style>
.stMultiSelect div[data-baseweb="select"] > div { flex-wrap: wrap; }
.stMultiSelect [data-baseweb="tag"] {
    max-width: 140px !important;
    min-width: auto !important;
    display: inline-flex !important;
    align-items: center !important;
}
</style>
""",
    unsafe_allow_html=True,
)

SESSION_DEFAULTS = {
    "progress_df": None,
    "progress_raw_df": None,
    "uploaded_filename": None,
    "progress_dir_cache": {},
    "progress_data_cache": {},
    "progress_data_source": "ğŸ“ ä»æ–‡ä»¶å¤¹é€‰æ‹©",
    "progress_folder_path": DEFAULT_DATA_FOLDER,
}
for key, default in SESSION_DEFAULTS.items():
    if key not in st.session_state:
        st.session_state[key] = default


def _render_load_saved_progress(as_expander: bool = False, show_details: bool = False) -> None:
    """åŠ è½½å·²ä¿å­˜çš„è¿›åº¦æ•°æ®ï¼ˆæœ¬åœ°ç¼“å­˜ï¼‰"""
    container = st.expander("ğŸ“‚ åŠ è½½å†å²è¿›åº¦æ•°æ®", expanded=False) if as_expander else st.container()
    with container:
        st.markdown("**ğŸ“‚ åŠ è½½å†å²è¿›åº¦æ•°æ®**")

        def _on_load(df: pd.DataFrame, metadata, extra_data):
            # æ¢å¤åˆ°å½“å‰é¡µé¢çš„ç¼“å­˜
            shell_col = _pick_column(df, ["å£³ä½“å·", "å£³ä½“ç¼–ç ", "å£³ä½“", "è…”ä½“å·", "è…”ä½“ç¼–å·", "Shell ID", "ShellID", "SN", "åºåˆ—å·"])
            if shell_col and shell_col in df.columns:
                df[shell_col] = df[shell_col].fillna("").astype(str).str.strip()

            st.session_state.progress_df = df
            st.session_state.progress_raw_df = df
            st.session_state.uploaded_filename = metadata.name
            st.session_state.progress_data_source = "ğŸ“ ä»æ–‡ä»¶å¤¹é€‰æ‹©"
            st.session_state.progress_loaded_id = metadata.id

        result = render_load_selector(
            category=DataCategory.PROGRESS,
            key="progress_load_inline" if not as_expander else "progress_load_expander",
            show_details=show_details,
            on_load_callback=_on_load,
        )

        if result:
            st.success("å·²åŠ è½½å†å²è¿›åº¦æ•°æ®")
            st.rerun()


def _load_from_folder() -> None:
    """Render folder selection and load data if requested."""
    st.markdown("**æ–‡ä»¶å¤¹è·¯å¾„**")
    col_path, col_refresh = st.columns([5, 1], vertical_alignment="center")
    with col_path:
        folder_path = st.text_input(
            "",
            placeholder=f"é»˜è®¤: {DEFAULT_DATA_FOLDER}",
            key="progress_folder_path",
            label_visibility="collapsed",
        )
    with col_refresh:
        refresh_btn = st.button("ğŸ”„", use_container_width=True, help="åˆ·æ–°æ–‡ä»¶åˆ—è¡¨")

    if not folder_path:
        st.toast("è¯·è¾“å…¥åŒ…å« WIP æŠ¥è¡¨çš„æ–‡ä»¶å¤¹è·¯å¾„", icon="â„¹ï¸")
        return

    try:
        search_path = resolve_input_path(folder_path)
    except ValueError as e:
        st.error(str(e))
        return

    if not search_path.exists() or not search_path.is_dir():
        st.error(f"è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶å¤¹: {search_path}")
        return

    excel_files = list(search_path.glob("*.xlsx")) + list(search_path.glob("*.xls"))
    csv_files = list(search_path.glob("*.csv"))
    all_files = sorted(excel_files + csv_files, key=lambda x: x.stat().st_mtime, reverse=True)

    if not all_files:
        st.warning(f"åœ¨ `{search_path}` ä¸­æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")
        return

    wip_files = [f for f in all_files if any(keyword in f.name or keyword.lower() in f.name.lower() for keyword in WIP_REPORT_KEYWORDS)]
    display_files = wip_files if wip_files else all_files
    max_display = 200
    display_files = display_files[:max_display]

    _dir_key = str(search_path)
    _dir_cache = st.session_state.progress_dir_cache.get(_dir_key, {})
    file_display_map = {}
    for f in display_files:
        fp = str(f)
        mtime = f.stat().st_mtime
        meta = _dir_cache.get(fp)
        if not meta or meta.get("mtime") != mtime:
            size_kb = f.stat().st_size / 1024.0
            _dir_cache[fp] = {"mtime": mtime, "size_kb": size_kb}
        else:
            size_kb = meta["size_kb"]
        file_display_map[f"{f.name} ({size_kb:.1f} KB)"] = fp
    st.session_state.progress_dir_cache[_dir_key] = _dir_cache

    st.markdown("**é€‰æ‹©æ–‡ä»¶ (å·²ç­›é€‰WIPæŠ¥è¡¨)**" if wip_files else "**é€‰æ‹©æ–‡ä»¶**")
    col_select, col_load = st.columns([4, 1], vertical_alignment="center")
    with col_select:
        selected_file_display = st.selectbox(
            "",
            options=list(file_display_map.keys()),
            key="progress_file_select",
            label_visibility="collapsed",
        )
    with col_load:
        load_btn = st.button("ğŸ“‚ åŠ è½½", type="primary", use_container_width=True)

    if not selected_file_display:
        return

    selected_file_path = file_display_map[selected_file_display]
    auto_load = st.session_state.progress_df is None and bool(wip_files)

    if load_btn or auto_load or refresh_btn:
        p = Path(selected_file_path)
        cache_key = f"{p.resolve()}::{p.stat().st_mtime}"
        cached = st.session_state.progress_data_cache.get(cache_key)

        if cached:
            df, cached_progress_df = cached
            st.session_state.progress_raw_df = df
            st.session_state.progress_df = cached_progress_df
            st.session_state.uploaded_filename = p.name
            st.success(f"å·²ä»ç¼“å­˜åŠ è½½ï¼å…± {len(df)} æ¡è®°å½•")
            if auto_load:
                st.rerun()
            return

        with st.spinner(f"æ­£åœ¨åŠ è½½ {p.name}..."):
            try:
                if p.suffix.lower() == ".csv":
                    header_df = pd.read_csv(p, nrows=0)
                    usecols = _compute_usecols(list(header_df.columns))
                    time_cols = [f"{ec}æ—¶é—´" for ec in STATION_MAPPING.keys() if f"{ec}æ—¶é—´" in header_df.columns]
                    dtype_map = {c: "string" for c in ["å£³ä½“å·", "æ–™å·", "ç”Ÿäº§è®¢å•"] if c in usecols}
                    df = pd.read_csv(p, usecols=usecols, dtype=dtype_map, parse_dates=time_cols, low_memory=False)
                else:
                    header_df = pd.read_excel(p, nrows=0)
                    usecols = _compute_usecols(list(header_df.columns))
                    engine = "openpyxl" if p.suffix.lower() == ".xlsx" else None
                    df = pd.read_excel(p, usecols=usecols, engine=engine)
                    time_cols = [f"{ec}æ—¶é—´" for ec in STATION_MAPPING.keys() if f"{ec}æ—¶é—´" in header_df.columns]
                    if time_cols:
                        df[time_cols] = df[time_cols].apply(pd.to_datetime, errors="coerce")

                # ç¡®ä¿å£³ä½“å·åˆ—ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… Arrow åºåˆ—åŒ–å¤±è´¥
                shell_col = _pick_column(df, ["å£³ä½“å·", "å£³ä½“ç¼–ç ", "å£³ä½“", "è…”ä½“å·", "è…”ä½“ç¼–å·", "Shell ID", "ShellID", "SN", "åºåˆ—å·"])
                if shell_col and shell_col in df.columns:
                    df[shell_col] = df[shell_col].fillna("").astype(str).str.strip()

                st.session_state.progress_raw_df = df
                st.session_state.progress_df = extract_progress_data(df)
                st.session_state.uploaded_filename = p.name
                st.success(f"åŠ è½½æˆåŠŸï¼å…± {len(df)} æ¡è®°å½•")
                st.session_state.progress_data_cache[cache_key] = (df, st.session_state.progress_df)
                if auto_load:
                    st.rerun()
            except Exception as e:
                st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")


def _load_from_upload() -> None:
    uploaded_file = st.file_uploader("ä¸Šä¼  WIP æ–‡ä»¶", type=["csv", "xls", "xlsx"], key="progress_uploader")
    if uploaded_file is None:
        return

    if st.session_state.uploaded_filename == uploaded_file.name:
        return

    with st.spinner("æ­£åœ¨è§£ææ–‡ä»¶..."):
        df = parse_uploaded_file(uploaded_file)
    if df is not None:
        # ç¡®ä¿å£³ä½“å·åˆ—ä¸ºå­—ç¬¦ä¸²ï¼Œé¿å… Arrow åºåˆ—åŒ–å¤±è´¥
        shell_col = _pick_column(df, ["å£³ä½“å·", "å£³ä½“ç¼–ç ", "å£³ä½“", "è…”ä½“å·", "è…”ä½“ç¼–å·", "Shell ID", "ShellID", "SN", "åºåˆ—å·"])
        if shell_col and shell_col in df.columns:
            df[shell_col] = df[shell_col].fillna("").astype(str).str.strip()

        st.session_state.progress_raw_df = df
        st.session_state.progress_df = extract_progress_data(df)
        st.session_state.uploaded_filename = uploaded_file.name
        st.session_state.progress_loaded_id = None
        st.success(f"æ–‡ä»¶è§£ææˆåŠŸï¼å…± {len(df)} æ¡è®°å½•")


def _render_filter_section(df: pd.DataFrame) -> Tuple[pd.DataFrame, Optional[set]]:
    """Render production order filter and return filtered DataFrame."""
    if df is None or df.empty:
        return df, None

    filtered_df = df.copy()
    selected_order_values = None
    
    if "ç”Ÿäº§è®¢å•" in filtered_df.columns:
        order_series = filtered_df["ç”Ÿäº§è®¢å•"].dropna().astype(str).str.strip()
        order_series = order_series[order_series != ""]
        order_options = sorted(order_series.unique().tolist(), reverse=True)

        if order_options:
            st.markdown("##### ğŸ” ç­›é€‰ç”Ÿäº§è®¢å•")
            col_sel, col_op = st.columns([4, 1], vertical_alignment="bottom")
            
            with col_sel:
                saved_selected = st.session_state.get("progress_production_orders")
                default_selected = saved_selected if saved_selected is not None else []
                
                selected_orders = st.multiselect(
                    "é€‰æ‹©ç”Ÿäº§è®¢å•",
                    options=order_options,
                    default=default_selected,
                    key="progress_production_orders",
                    placeholder="å¯è¾“å…¥æœç´¢å¹¶é€‰æ‹©è®¢å•ï¼ˆæ”¯æŒå¤šé€‰ï¼‰",
                    label_visibility="collapsed",
                )
            
            with col_op:
                 # ä½¿ç”¨ columns æ”¾ç½®å°æŒ‰é’®
                 c1, c2 = st.columns(2)
                 
                 def _select_all_orders():
                     st.session_state["progress_production_orders"] = order_options
                     
                 def _clear_all_orders():
                     st.session_state["progress_production_orders"] = []

                 with c1:
                     st.button("å…¨é€‰", key="progress_order_select_all_btn_new", use_container_width=True, on_click=_select_all_orders)
                 with c2:
                     st.button("æ¸…ç©º", key="progress_order_clear_btn_new", use_container_width=True, on_click=_clear_all_orders)
            
            if selected_orders:
                selected_order_values = {order.strip() for order in selected_orders}
                filtered_df = filtered_df[
                    filtered_df["ç”Ÿäº§è®¢å•"].fillna("").astype(str).str.strip().isin(selected_order_values)
                ]
            else:
                selected_order_values = None
        else:
            st.caption("æœªæ£€æµ‹åˆ°ç”Ÿäº§è®¢å•æ•°æ®")
    
    return filtered_df, selected_order_values


def _render_save_section(filtered_df: pd.DataFrame):
    if filtered_df.empty:
        st.info("æš‚æ— æ•°æ®å¯ä¿å­˜")
        return

    # è·å–ç”Ÿäº§è®¢å•åˆ—è¡¨
    production_orders = []
    if "ç”Ÿäº§è®¢å•" in filtered_df.columns:
        production_orders = filtered_df["ç”Ÿäº§è®¢å•"].dropna().astype(str).str.strip().unique().tolist()
        production_orders = [o for o in production_orders if o]
    
    # äº§å“ç±»å‹åç§°è¾“å…¥
    default_name = ""
    if production_orders:
        default_name = production_orders[0] if len(production_orders) == 1 else f"{production_orders[0]} ç­‰{len(production_orders)}ä¸ªè®¢å•"
    
    col_save, col_update = st.columns(2, vertical_alignment="top")
    
    with col_save:
        st.markdown("#### ğŸ’¾ ä¿å­˜æ•°æ®")
        product_type_name = st.text_input(
            "æ•°æ®åç§° / äº§å“ç±»å‹åç§°",
            value=default_name,
            placeholder="è¾“å…¥åç§°ï¼ˆå¦‚ M20-AM-Cï¼‰",
            key="progress_product_type_name",
            help="å°†ä½œä¸ºå†å²æ•°æ®é›†åç§°å’Œ Data Manager äº§å“ç±»å‹åç§°"
        )
        save_clicked = st.button("ğŸ’¾ ä¿å­˜", key="progress_save_combined_btn", use_container_width=True, type="primary")
    
    with col_update:
        st.markdown("#### ğŸ”„ æ›´æ–°æ•°æ®")

    if save_clicked:
        if not product_type_name or not product_type_name.strip():
            st.error("âŒ è¯·è¾“å…¥äº§å“ç±»å‹åç§°")
            return
        
        save_name = product_type_name.strip()
        source_path = st.session_state.get("uploaded_filename") or st.session_state.get("progress_folder_path")
        dataset_id = None
        product_type_id = None

        # å‡†å¤‡å­˜å‚¨æœåŠ¡
        store = st.session_state.get("local_data_store")
        if store is None:
            store = LocalDataStore()
            st.session_state["local_data_store"] = store
        service = get_product_type_service()

        # æ£€æŸ¥é‡åï¼ˆå†å²+æ•°æ®ç®¡ç†å™¨ï¼‰ï¼Œéœ€è¦ç¡®è®¤è¦†ç›–
        existing_history = [ds for ds in store.list_datasets(category=DataCategory.PROGRESS) if ds.name == save_name]
        existing_product_types = [pt for pt in service.list_product_types() if pt.name == save_name]
        overwrite_allowed = (
            st.session_state.get("progress_overwrite_confirmed")
            and st.session_state.get("progress_overwrite_name") == save_name
        )
        if existing_history or existing_product_types:
            if not overwrite_allowed:
                st.warning(f"âš ï¸ åç§°å·²å­˜åœ¨ï¼š{save_name}ã€‚é€‰æ‹©è¦†ç›–å°†åˆ é™¤åŒåå†å²å¹¶æ›´æ–°äº§å“ç±»å‹ã€‚")
                if st.button("âœ… ç¡®è®¤è¦†ç›–", key="progress_overwrite_confirm_btn", use_container_width=True, type="primary"):
                    st.session_state["progress_overwrite_confirmed"] = True
                    st.session_state["progress_overwrite_name"] = save_name
                    overwrite_allowed = True
                else:
                    st.info("å¦‚éœ€å–æ¶ˆè¦†ç›–ï¼Œè¯·ä¿®æ”¹åç§°åé‡æ–°ä¿å­˜ã€‚")
                    return

        try:
            # ä¿å­˜åˆ°æœ¬åœ°å†å²ï¼ˆä¾›â€œåŠ è½½å†å²â€ä½¿ç”¨ï¼‰
            for ds in existing_history:
                try:
                    store.delete(ds.id)
                except Exception:
                    # å¿½ç•¥å•ä¸ªåˆ é™¤å¤±è´¥ï¼Œç»§ç»­å°è¯•ä¿å­˜æ–°æ•°æ®
                    pass
            dataset_id = store.save(
                df=filtered_df,
                category=DataCategory.PROGRESS,
                name=save_name,
                custom_filename=save_name,
                source_file=source_path,
            )
            st.session_state.progress_loaded_id = dataset_id

            # å‡†å¤‡å£³ä½“æ•°æ®å¹¶ä¿å­˜åˆ°æ•°æ®ç®¡ç†å™¨
            shells_df = prepare_shells_dataframe_for_data_manager(filtered_df)
            # è¦†ç›–ä½¿ç”¨ upsertï¼Œé¿å…ç”Ÿæˆé‡å¤äº§å“ç±»å‹
            if existing_product_types:
                product_type_id = service.upsert_product_type(
                    name=save_name,
                    shells_df=shells_df,
                    production_orders=production_orders,
                    source_file=source_path,
                )
            else:
                product_type_id = service.save_product_type(
                    name=save_name,
                    shells_df=shells_df,
                    production_orders=production_orders,
                    source_file=source_path,
                )

            st.toast(f"âœ… å·²ä¿å­˜åˆ°å†å²ä¸æ•°æ®ç®¡ç†å™¨ï¼š{save_name}")
            st.caption(f"å†å²ID: {dataset_id[:8]}... | äº§å“ç±»å‹ID: {product_type_id[:8]}...")

            # è¦†ç›–åé‡ç½® Data Manager ç›¸å…³ç¼“å­˜ï¼Œç¡®ä¿ä¸‹æ¬¡åŠ è½½è¯»å–æœ€æ–°å£³ä½“æ•°æ®
            for key in [
                "dm_shells_df",
                "dm_shell_progress_list",
                "dm_shell_cache_key",
                "dm_gantt_data",
                "dm_analysis_df",
            ]:
                st.session_state[key] = None
            st.session_state["dm_thresholds"] = {}
            st.session_state["dm_selected_product_type_id"] = product_type_id
            st.session_state["dm_selected_product_type_ids"] = [product_type_id]
            st.session_state["dm_selected_product_type_name"] = save_name
            st.session_state["dm_selected_orders"] = []
        except ValueError as e:
            st.error(f"âŒ ä¿å­˜å¤±è´¥: {str(e)}")
            if dataset_id and not product_type_id:
                st.info(f"å†å²æ•°æ®é›†å·²ä¿å­˜ (ID: {dataset_id[:8]}...)ï¼Œä½†æ•°æ®ç®¡ç†å™¨ä¿å­˜æœªå®Œæˆ")
        except StorageError as e:
            st.error(f"âŒ æœ¬åœ°ä¿å­˜å¤±è´¥: {str(e)}")
        except Exception as e:
            st.error(f"âŒ ä¿å­˜æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            if dataset_id and not product_type_id:
                st.info(f"å†å²æ•°æ®é›†å·²ä¿å­˜ (ID: {dataset_id[:8]}...)ï¼Œä½†æ•°æ®ç®¡ç†å™¨ä¿å­˜æœªå®Œæˆ")
        finally:
            # é‡ç½®è¦†ç›–ç¡®è®¤çŠ¶æ€
            st.session_state["progress_overwrite_confirmed"] = False
            st.session_state["progress_overwrite_name"] = None

    # æ›´æ–°å·²æœ‰æ•°æ®é›†ï¼ˆåªæ›´æ–°å½“å‰å£³ä½“çš„ç«™åˆ«/çŠ¶æ€ï¼‰
    with col_update:
        store = st.session_state.get("local_data_store")
        if store is None:
            store = LocalDataStore()
            st.session_state["local_data_store"] = store

        existing_datasets = store.list_datasets(category=DataCategory.PROGRESS)
        if not existing_datasets:
            st.info("æš‚æ— å¯æ›´æ–°çš„å†å²æ•°æ®é›†")
            return

        option_map = {
            f"{meta.name}ï¼ˆ{meta.row_count}è¡Œ | {meta.created_at[:16]}ï¼‰": meta for meta in existing_datasets
        }
        selected_label = st.selectbox("é€‰æ‹©è¦æ›´æ–°çš„æ•°æ®é›†", list(option_map.keys()), key="progress_update_select")
        update_clicked = st.button("ğŸ”„ æ›´æ–°åˆ°å·²æœ‰æ•°æ®é›†", key="progress_update_btn", use_container_width=True, type="secondary")

        if update_clicked:
            target_meta = option_map.get(selected_label)
            if not target_meta:
                st.error("æœªæ‰¾åˆ°é€‰ä¸­çš„æ•°æ®é›†")
                return

            shell_candidates = ["å£³ä½“å·", "å£³ä½“ç¼–ç ", "å£³ä½“", "è…”ä½“å·", "è…”ä½“ç¼–å·", "Shell ID", "ShellID", "SN", "åºåˆ—å·"]
            shell_col_new = _pick_column(filtered_df, shell_candidates)
            if not shell_col_new:
                st.error("å½“å‰æ•°æ®ç¼ºå°‘å£³ä½“åˆ—ï¼Œæ— æ³•æ›´æ–°")
                return

            try:
                df_old, meta_old, extra_old = store.load(target_meta.id)
            except Exception as e:
                st.error(f"åŠ è½½ç›®æ ‡æ•°æ®é›†å¤±è´¥: {e}")
                return

            shell_col_old = _pick_column(df_old, shell_candidates)
            target_shell_col = shell_col_old or shell_col_new

            # å½’ä¸€åŒ–å£³ä½“åˆ—
            df_new = filtered_df.rename(columns={shell_col_new: target_shell_col}) if shell_col_new != target_shell_col else filtered_df.copy()
            df_new[target_shell_col] = df_new[target_shell_col].fillna("").astype(str).str.strip()
            df_old[target_shell_col] = df_old[target_shell_col].fillna("").astype(str).str.strip()

            # å¯¹é½åˆ—ï¼Œä¿ç•™æ—§æ•°æ®ä¸­æœªè¦†ç›–çš„å£³ä½“
            all_columns = list({*df_old.columns, *df_new.columns})
            df_old = df_old.reindex(columns=all_columns)
            df_new = df_new.reindex(columns=all_columns)

            new_shells = set(df_new[target_shell_col])
            df_old_kept = df_old[~df_old[target_shell_col].isin(new_shells)]
            combined = pd.concat([df_old_kept, df_new], ignore_index=True)

            try:
                store.delete(target_meta.id)
                updated_id = store.save(
                    df=combined,
                    category=DataCategory.PROGRESS,
                    name=target_meta.name,
                    custom_filename=target_meta.name,
                    note=target_meta.note,
                    extra_data=extra_old,
                    source_file=target_meta.source_file,
                )
                # åŒæ­¥æ›´æ–° Data Manager ä¸­çš„äº§å“ç±»å‹æ•°æ®
                shells_df_combined = prepare_shells_dataframe_for_data_manager(combined)
                orders_combined: List[str] = []
                if "ç”Ÿäº§è®¢å•" in combined.columns:
                    orders_combined = (
                        combined["ç”Ÿäº§è®¢å•"].dropna().astype(str).str.strip().unique().tolist()
                    )
                service = get_product_type_service()
                dm_product_type_id = service.upsert_product_type(
                    name=target_meta.name,
                    shells_df=shells_df_combined,
                    production_orders=orders_combined,
                    source_file=target_meta.source_file,
                )
                st.toast(f"âœ… å·²æ›´æ–°æ•°æ®é›†ä¸ Data Managerï¼š{target_meta.name}")
                st.caption(f"æ–°æ•°æ®é›†ID: {updated_id[:8]}... | äº§å“ç±»å‹ID: {dm_product_type_id[:8]}...")
            except Exception as e:
                st.error(f"æ›´æ–°å¤±è´¥: {e}")


# ============================================================================
# Main Layout
# ============================================================================

with st.container(border=True):
    st.markdown("### ğŸ“‚ æ•°æ®ç®¡ç†")

    action_mode = st.radio(
        "æ“ä½œ",
        options=["ğŸ“ ä»æ–‡ä»¶å¤¹é€‰æ‹©", "ğŸ“¤ ä¸Šä¼ æ–‡ä»¶", "ğŸ“œ åŠ è½½å†å²", "ğŸ’¾ ä¿å­˜æ•°æ®"],
        horizontal=True,
        label_visibility="collapsed",
        key="progress_action_mode",
    )
    save_mode_selected = action_mode == "ğŸ’¾ ä¿å­˜æ•°æ®"

    # æ‰§è¡Œé€‰ä¸­çš„æ“ä½œ
    if action_mode == "ğŸ“ ä»æ–‡ä»¶å¤¹é€‰æ‹©":
        _load_from_folder()
    elif action_mode == "ğŸ“¤ ä¸Šä¼ æ–‡ä»¶":
        _load_from_upload()
    elif action_mode == "ğŸ“œ åŠ è½½å†å²":
        _render_load_saved_progress(as_expander=False, show_details=True)

    # åŠ è½½åç»Ÿä¸€ç­›é€‰
    filtered_progress_df = pd.DataFrame()
    selected_order_values = None
    if st.session_state.progress_df is not None:
        filtered_progress_df, selected_order_values = _render_filter_section(st.session_state.progress_df)
    
    # ä¿å­˜æ—¶ä½¿ç”¨å½“å‰ç­›é€‰åçš„æ•°æ®é›†
    if save_mode_selected:
        if st.session_state.progress_df is not None:
            _render_save_section(filtered_progress_df)
        else:
            st.info("è¯·å…ˆåŠ è½½æ•°æ®ï¼Œå†ä¿å­˜")

# ä½¿ç”¨ session_state ä¸­çš„æ•°æ®
if st.session_state.progress_df is not None:
    progress_df = st.session_state.progress_df
    df_raw = st.session_state.progress_raw_df

    # Apply filter to df_raw if needed (Logic preserved from original)
    if selected_order_values is not None and df_raw is not None:
        production_order_column = progress_df.attrs.get("production_order_column")
        if production_order_column and production_order_column in df_raw.columns:
            preview_series = df_raw[production_order_column].fillna("").astype(str).str.strip()
            df_raw = df_raw[preview_series.isin(selected_order_values)] if selected_order_values else df_raw.iloc[0:0]
    elif not selected_order_values and df_raw is not None:
        # If no filter selected, clear raw df (as per original logic)
        df_raw = df_raw.iloc[0:0]

    if filtered_progress_df.empty:
        if st.session_state.progress_df is not None and not st.session_state.progress_df.empty:
             st.warning("ç­›é€‰æ¡ä»¶ä¸‹æ²¡æœ‰æ•°æ®ï¼Œè¯·è°ƒæ•´ç”Ÿäº§è®¢å•é€‰æ‹©")
    else:
        # Metrics and Charts
        col1, col2, col3, col4 = st.columns([1, 1.2, 1, 1.5])
        with col1:
            st.metric("å£³ä½“æ€»æ•°", len(filtered_progress_df))
        with col2:
            if "å®Œæˆç«™åˆ«" in filtered_progress_df.columns:
                avg_progress = filtered_progress_df["å®Œæˆç«™åˆ«"].apply(len).mean()
                st.metric("å¹³å‡å®Œæˆç«™åˆ«æ•°", f"{avg_progress:.1f}")
        with col3:
            st.metric("åŸºç¡€ç«™åˆ«æ•°", len(BASE_STATIONS))
        with col4:
            latest_time = None
            time_cols = progress_df.attrs.get("time_cols", [])
            if df_raw is not None and time_cols:
                tc = [c for c in time_cols if c in df_raw.columns]
                if tc:
                    parsed = df_raw[tc].apply(pd.to_datetime, errors="coerce")
                    max_val = parsed.max().max()
                    if pd.notna(max_val):
                        latest_time = max_val
            if latest_time:
                st.metric("æœ€æ–°æµ‹è¯•æ—¶é—´", latest_time.strftime("%Y-%m-%d %H:%M"))
            else:
                st.metric("æœ€æ–°æµ‹è¯•æ—¶é—´", "æ— æ•°æ®")

        counts_df = calculate_station_counts(filtered_progress_df)
        if not counts_df.empty:
            st.markdown("---")
            st.markdown("### å„ç«™åˆ«å½“å‰æ•°é‡")
            table_col, chart_col = st.columns([2, 3])

            with table_col:
                counts_style = counts_df.style.format({"å æ¯”": "{:.1%}"})
                table_height = max(180, min(320, 36 * len(counts_df) + 60))
                st.dataframe(counts_style, use_container_width=True, height=table_height)

            with chart_col:
                station_order = counts_df["ç«™åˆ«"].tolist()
                chart_height = max(160, min(360, 28 * len(counts_df)))
                chart = (
                    alt.Chart(counts_df)
                    .mark_bar(cornerRadius=8, opacity=0.9, strokeWidth=1.5)
                    .encode(
                        x=alt.X("æ•°é‡:Q", title="å®Œæˆæ•°é‡", axis=alt.Axis(grid=True, gridOpacity=0.2, tickMinStep=1)),
                        y=alt.Y(
                            "ç«™åˆ«:N",
                            sort=station_order,
                            title="ç«™åˆ«",
                            axis=alt.Axis(labelFontSize=12, labelFontWeight="bold"),
                        ),
                        color=alt.Color(
                            "æ•°é‡:Q",
                            scale=alt.Scale(scheme="blues", domain=[counts_df["æ•°é‡"].min(), counts_df["æ•°é‡"].max()]),
                            legend=None,
                        ),
                        stroke=alt.value("#ffffff33"),
                        tooltip=["ç«™åˆ«", "æ•°é‡", alt.Tooltip("å æ¯”:Q", title="å æ¯”", format=".1%")],
                    )
                ).properties(height=chart_height).configure_view(strokeWidth=0).configure_axis(
                    titleFontSize=13, titleFontWeight="bold"
                )
                st.altair_chart(chart, use_container_width=True, theme="streamlit")

        engineering_df = filtered_progress_df[filtered_progress_df["æ˜¯å¦å·¥ç¨‹åˆ†æ"] == True]
        if not engineering_df.empty:
            st.markdown("---")
            st.markdown("### ğŸ” å·¥ç¨‹åˆ†æç«™åˆ«åˆ†å¸ƒ")

            engineering_stations = []
            for _, row in engineering_df.iterrows():
                last_station = row.get("ä¸Šä¸€ç«™", "")
                last_station_normalized = normalize_station_name(last_station)
                if last_station_normalized:
                    engineering_stations.append(last_station_normalized)

            if engineering_stations:
                engineering_counts = pd.Series(engineering_stations).value_counts().reset_index()
                engineering_counts.columns = ["ç«™åˆ«", "æ•°é‡"]
                engineering_counts["å æ¯”"] = engineering_counts["æ•°é‡"] / engineering_counts["æ•°é‡"].sum()

                eng_table_col, eng_chart_col = st.columns([2, 3])
                with eng_table_col:
                    st.caption(f"å·¥ç¨‹åˆ†ææ€»æ•°: {len(engineering_df)} ä¸ª")
                    eng_counts_style = engineering_counts.style.format({"å æ¯”": "{:.1%}"})
                    st.dataframe(eng_counts_style, use_container_width=True, hide_index=True)

                with eng_chart_col:
                    st.caption("å·¥ç¨‹åˆ†æç«™åˆ«å æ¯”")
                    # æ‚¬åœé«˜äº®æ•ˆæœ
                    hover = alt.selection_point(fields=["ç«™åˆ«"], on="pointerover", empty=False)
                    pie_chart = (
                        alt.Chart(engineering_counts)
                        .mark_arc(innerRadius=20, outerRadius=70)
                        .encode(
                            theta=alt.Theta("æ•°é‡:Q", stack=True),
                            color=alt.Color("ç«™åˆ«:N", legend=alt.Legend(title="ç«™åˆ«", orient="right"), scale=alt.Scale(scheme="category20")),
                            tooltip=[
                                alt.Tooltip("ç«™åˆ«:N", title="ç«™åˆ«"),
                                alt.Tooltip("æ•°é‡:Q", title="æ•°é‡"),
                                alt.Tooltip("å æ¯”:Q", title="å æ¯”", format=".1%"),
                            ],
                            opacity=alt.condition(hover, alt.value(1), alt.value(0.6)),
                            stroke=alt.condition(hover, alt.value("#333"), alt.value(None)),
                            strokeWidth=alt.condition(hover, alt.value(2), alt.value(0)),
                        )
                        .add_params(hover)
                        .properties(height=180)
                    )
                    st.altair_chart(pie_chart, use_container_width=True)

        st.markdown("---")
        st.markdown("### ğŸ“‹ è¿›åº¦è¡¨æ ¼")
        show_eng_only = st.checkbox("ğŸ” ä»…æ˜¾ç¤ºå·¥ç¨‹åˆ†æçš„å£³ä½“", value=False, key="progress_show_eng_only")
        source_df = filtered_progress_df[filtered_progress_df["æ˜¯å¦å·¥ç¨‹åˆ†æ"] == True] if show_eng_only else filtered_progress_df
        table_df = create_progress_table(source_df)
        st.dataframe(table_df, use_container_width=True, height=400)

    with st.expander("ğŸ“„ æŸ¥çœ‹åŸå§‹æ•°æ®"):
        st.dataframe(df_raw.head(20), use_container_width=True)
else:
    st.info(
        """
        ### ğŸ“– ä½¿ç”¨è¯´æ˜

        1. **ä¸Šä¼ æ–‡ä»¶**ï¼šç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ä¸Šä¼ åŒ…å«å£³ä½“è¿›åº¦ä¿¡æ¯çš„ Excel æˆ– CSV æ–‡ä»¶  
        2. **ä»æ–‡ä»¶å¤¹åŠ è½½**ï¼šè¾“å…¥æ•°æ®æ–‡ä»¶å¤¹è·¯å¾„ï¼Œé€‰æ‹©å¹¶åŠ è½½ WIP æŠ¥è¡¨  
        3. **æŸ¥çœ‹ç»“æœ**ï¼š
           - ç»Ÿè®¡å›¾ï¼šå±•ç¤ºå„ç«™åˆ«å½“å‰æ•°é‡å’Œå æ¯”  
           - è¿›åº¦è¡¨æ ¼ï¼šåˆ—å‡ºæ¯ä¸ªå£³ä½“çš„å®Œæˆæƒ…å†µ  
        """
    )
