# title: è¿›åº¦è¿½è¸ª
import streamlit as st
import pandas as pd
import altair as alt
from datetime import datetime
from typing import List, Dict, Optional
import io
import sys
from pathlib import Path
import time


# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„ä»¥å¯¼å…¥config

sys.path.insert(0, str(Path(__file__).parent.parent))
from config import DEFAULT_DATA_FOLDER, WIP_REPORT_KEYWORDS
APP_ROOT = Path(__file__).resolve().parent.parent
def resolve_input_path(path_str: str) -> Path:
    """Resolve user-provided folder path, supporting relative inputs like ./data."""
    normalized = path_str.strip()
    if not normalized:
        raise ValueError("è·¯å¾„ä¸èƒ½ä¸ºç©º")

    candidate = Path(normalized).expanduser()
    if not candidate.is_absolute():
        candidate = APP_ROOT / candidate
    return candidate.resolve()





# å®šä¹‰æ‰€æœ‰ç«™åˆ«ï¼ˆæŒ‰å·¥è‰ºæµç¨‹é¡ºåºï¼‰- åŸºç¡€ç«™åˆ«

BASE_STATIONS = [
    "æ‰“æ ‡", "æ¸…æ´—", "å£³ä½“ç»„è£…", "å›æµ", "facå‰å¤‡æ–™", "æ‰“çº¿", "fac", "facè¡¥èƒ¶", 
    "facè¡¥èƒ¶åçƒ˜çƒ¤", "facæµ‹è¯•", "sacç»„è£…", "å…‰çº¤ç»„è£…", "å…‰çº¤ç»„è£…åçƒ˜çƒ¤", 
    "çº¢å…‰è€¦åˆ", "è£…å¤§å", "çº¢å…‰è€¦åˆåçƒ˜çƒ¤", "åˆæŸ", "åˆæŸåçƒ˜çƒ¤", 
    "NAå‰é•œæ£€", "NAå‰çº¢å…‰ç«¯é¢æ£€", "NAæµ‹è¯•", "è€¦åˆæµ‹è¯•", "è¡¥èƒ¶", "æ¸©åº¦å¾ªç¯", 
    "Preæµ‹è¯•", "ä½æ¸©å­˜å‚¨", "ä½æ¸©å­˜å‚¨åæµ‹è¯•", "é«˜æ¸©å­˜å‚¨", "é«˜æ¸©å­˜å‚¨åæµ‹è¯•", 
    "è€åŒ–å‰çº¢å…‰ç«¯é¢", "postæµ‹è¯•", "çº¢å…‰ç«¯é¢æ£€æŸ¥", "é•œæ£€", "å°ç›–", "å°ç›–æµ‹è¯•", 
    "åˆ†çº§", "å…¥åº“æ£€", "å…¥åº“", "RMA"
]

def get_stations_for_part(part_number: str) -> list:
    """æ ¹æ®æ–™å·è¿”å›é€‚ç”¨çš„ç«™åˆ«åˆ—è¡¨"""
    stations = BASE_STATIONS.copy()
    # å¦‚æœæ–™å·åŒ…å«Vï¼Œåœ¨åˆæŸåçƒ˜çƒ¤åé¢æ’å…¥VBGå’ŒVBGåçƒ˜çƒ¤
    if 'V' in str(part_number).upper():
        hesu_idx = stations.index("åˆæŸåçƒ˜çƒ¤")
        stations.insert(hesu_idx + 1, "VBG")
        stations.insert(hesu_idx + 2, "VBGåçƒ˜çƒ¤")
    # æ·»åŠ "å·²å®Œæˆ"ä½œä¸ºæœ€åä¸€ä¸ªç«™åˆ«
    stations.append("å·²å®Œæˆ")
    return stations
# ç«™åˆ«æ˜ å°„ï¼ˆExcelåˆ—ååˆ°æ ‡å‡†ç«™åˆ«åï¼‰

STATION_MAPPING = {
    "æœºæ¢°ä»¶æ‰“æ ‡": "æ‰“æ ‡",
    "æœºæ¢°ä»¶æ¸…æ´—": "æ¸…æ´—",
    "å£³ä½“ç»„è£…": "å£³ä½“ç»„è£…",
    "å…‰è€¦å›æµ": "å›æµ",
    "FACå‰å¤‡æ–™": "facå‰å¤‡æ–™",
    "æ‰“çº¿": "æ‰“çº¿",
    "FAC": "fac",
    "FACè¡¥èƒ¶": "facè¡¥èƒ¶",
    "FACè¡¥èƒ¶åçƒ˜çƒ¤": "facè¡¥èƒ¶åçƒ˜çƒ¤",
    "FACæµ‹è¯•": "facæµ‹è¯•",
    "SACç»„è£…": "sacç»„è£…",
    "å…‰çº¤ç»„è£…": "å…‰çº¤ç»„è£…",
    "å…‰çº¤ç»„è£…åçƒ˜çƒ¤": "å…‰çº¤ç»„è£…åçƒ˜çƒ¤",
    "çº¢å…‰è€¦åˆ": "çº¢å…‰è€¦åˆ",
    "è£…å¤§å": "è£…å¤§å",
    "è€¦åˆåçƒ˜çƒ¤": "çº¢å…‰è€¦åˆåçƒ˜çƒ¤",
    "çº¢å…‰è€¦åˆåçƒ˜çƒ¤": "çº¢å…‰è€¦åˆåçƒ˜çƒ¤",
    "åˆæŸ": "åˆæŸ",
    "åˆæŸåçƒ˜çƒ¤": "åˆæŸåçƒ˜çƒ¤",
    "VBG": "VBG",
    "VBGåçƒ˜çƒ¤": "VBGåçƒ˜çƒ¤",
    "NAå‰é•œæ£€": "NAå‰é•œæ£€",
    "NAå‰çº¢å…‰ç«¯é¢æ£€": "NAå‰çº¢å…‰ç«¯é¢æ£€",
    "NAå‰çº¢å…‰ç«¯é¢æ£€æŸ¥": "NAå‰çº¢å…‰ç«¯é¢æ£€",
    "NAæµ‹è¯•": "NAæµ‹è¯•",
    "è€¦åˆæµ‹è¯•": "è€¦åˆæµ‹è¯•",
    "è¡¥èƒ¶": "è¡¥èƒ¶",
    "æ¸©åº¦å¾ªç¯": "æ¸©åº¦å¾ªç¯",
    "preæµ‹è¯•": "Preæµ‹è¯•",
    "Preæµ‹è¯•": "Preæµ‹è¯•",
    "ä½æ¸©å­˜å‚¨": "ä½æ¸©å­˜å‚¨",
    "ä½æ¸©å‚¨å­˜": "ä½æ¸©å­˜å‚¨",
    "ä½æ¸©å­˜å‚¨åæµ‹è¯•": "ä½æ¸©å­˜å‚¨åæµ‹è¯•",
    "ä½æ¸©å‚¨å­˜åæµ‹è¯•": "ä½æ¸©å­˜å‚¨åæµ‹è¯•",
    "é«˜æ¸©å­˜å‚¨": "é«˜æ¸©å­˜å‚¨",
    "é«˜æ¸©å­˜å‚¨åæµ‹è¯•": "é«˜æ¸©å­˜å‚¨åæµ‹è¯•",
    "è€åŒ–": "è€åŒ–å‰çº¢å…‰ç«¯é¢",
    "è€åŒ–å‰çº¢å…‰ç«¯é¢": "è€åŒ–å‰çº¢å…‰ç«¯é¢",
    "è€åŒ–å‰çº¢å…‰ç«¯é¢æ£€æŸ¥": "è€åŒ–å‰çº¢å…‰ç«¯é¢",
    "postæµ‹è¯•": "postæµ‹è¯•",
    "Postæµ‹è¯•": "postæµ‹è¯•",
    "çº¢å…‰ç«¯é¢æ£€æŸ¥": "çº¢å…‰ç«¯é¢æ£€æŸ¥",
    "é•œæ£€": "é•œæ£€",
    "å°ç›–": "å°ç›–",
    "å°ç›–æµ‹è¯•": "å°ç›–æµ‹è¯•",
    "é¡¶ç›–": "å°ç›–",
    "é¡¶ç›–æµ‹è¯•": "å°ç›–æµ‹è¯•",
    "åˆ†çº§": "åˆ†çº§",
    "å…¥åº“æ£€": "å…¥åº“æ£€",
    "å…¥åº“--å…‰è€¦": "å…¥åº“",
    "å…¥åº“": "å…¥åº“",
    "å¾…å…¥åº“": "å…¥åº“",
    "RMAæ€§èƒ½æµ‹è¯•": "RMA",
    "RMAæ‹†ç›–æ£€æŸ¥": "RMA",
    "RMA": "RMA",
    "æ‹†è§£": "å·¥ç¨‹åˆ†æ",
    "æœªå¼€å§‹": "æœªå¼€å§‹",
    "å·²å®Œæˆ": "å·²å®Œæˆ",
    "complete": "å·²å®Œæˆ",
    "COMPLETE": "å·²å®Œæˆ",
    "TERMINATED": "å·²å®Œæˆ",
    "å®Œæˆ": "å·²å®Œæˆ"
}

STATION_MAPPING_LOWER = {key.lower(): value for key, value in STATION_MAPPING.items()}
BASE_STATIONS_LOWER = {station.lower(): station for station in BASE_STATIONS}

PRODUCTION_ORDER_CANDIDATES: List[str] = [
    "ç”Ÿäº§è®¢å•",
    "ERPç”Ÿäº§è®¢å•",
    "SAPç”Ÿäº§è®¢å•",
    "ç”Ÿäº§è®¢å•å·",
    "è®¢å•å·",
    "å·¥å•å·",
]
def _compute_usecols(header_cols: List[str]) -> List[str]:
    cols_set = {str(c).strip() for c in header_cols}
    needed = {"å£³ä½“å·", "æ–™å·", "å½“å‰ç«™ç‚¹", "ä¸Šä¸€ç«™"}
    for name in PRODUCTION_ORDER_CANDIDATES:
        if name in cols_set:
            needed.add(name)
            break
    for excel_col in STATION_MAPPING.keys():
        time_col = f"{excel_col}æ—¶é—´"
        if time_col in cols_set:
            needed.add(time_col)
    return [c for c in header_cols if str(c).strip() in needed]

def parse_uploaded_file(uploaded_file) -> Optional[pd.DataFrame]:
    """è§£æä¸Šä¼ çš„æ–‡ä»¶"""
    try:
        if uploaded_file.name.endswith('.csv'):
            # Try UTF-8 first, then GBK for Chinese files

            try:

                df = pd.read_csv(uploaded_file, encoding='utf-8')

            except UnicodeDecodeError:

                uploaded_file.seek(0)  # Reset file pointer

                df = pd.read_csv(uploaded_file, encoding='gbk')

        elif uploaded_file.name.endswith(('.xls', '.xlsx')):

            df = pd.read_excel(uploaded_file)

        else:

            st.error("ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œè¯·ä¸Šä¼  CSV æˆ– Excel æ–‡ä»¶")

            return None

        return df

    except Exception as e:

        st.error(f"æ–‡ä»¶è§£æå¤±è´¥: {str(e)}")

        return None



def normalize_station_name(station_name: str) -> str:

    """å°†Excelä¸­çš„ç«™åˆ«åç§°æ ‡å‡†åŒ–ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰"""

    if pd.isna(station_name) or station_name == '':

        return ''



    station_name = str(station_name).strip()

    station_name_lower = station_name.lower()



    # åŒ…å« rma å­—æ ·çš„éƒ½å½’åˆ° RMA ç«™åˆ«

    if 'rma' in station_name_lower:

        return 'RMA'



    # ç›´æ¥æ˜ å°„ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰

    if station_name_lower in STATION_MAPPING_LOWER:

        return STATION_MAPPING_LOWER[station_name_lower]



    # å°è¯•å»æ‰"æµ‹è¯•"åç¼€

    if station_name.endswith('æµ‹è¯•') or station_name_lower.endswith('æµ‹è¯•'):

        base_name = station_name[:-2]

        base_name_lower = base_name.lower()

        if base_name_lower in STATION_MAPPING_LOWER:

            return STATION_MAPPING_LOWER[base_name_lower]



    # å°è¯•æ·»åŠ "æµ‹è¯•"åç¼€

    test_name_lower = station_name_lower + 'æµ‹è¯•'

    if test_name_lower in STATION_MAPPING_LOWER:

        return STATION_MAPPING_LOWER[test_name_lower]



    # å°è¯•åœ¨BASE_STATIONSä¸­æŸ¥æ‰¾ï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰

    if station_name_lower in BASE_STATIONS_LOWER:

        return BASE_STATIONS_LOWER[station_name_lower]



    # å°è¯•æ¨¡ç³ŠåŒ¹é…ï¼ˆå»é™¤ç©ºæ ¼ã€ç‰¹æ®Šå­—ç¬¦ï¼‰

    clean_name = station_name_lower.replace(' ', '').replace('-', '').replace('_', '')

    for key, value in STATION_MAPPING_LOWER.items():

        clean_key = key.replace(' ', '').replace('-', '').replace('_', '')

        if clean_name == clean_key:

            return value

   

    # è¿”å›åŸå§‹åç§°

    return station_name



def extract_progress_data(df: pd.DataFrame, light: bool = False) -> pd.DataFrame:

    """ä»åŸå§‹æ•°æ®ä¸­æå–è¿›åº¦ä¿¡æ¯"""

    progress_data = []

    unrecognized_stations = set()

    

    column_lookup = {str(col).strip(): col for col in df.columns}

    production_order_column = next(

        (column_lookup[name] for name in PRODUCTION_ORDER_CANDIDATES if name in column_lookup),

        None,

    )

    

    existing_station_time_cols = [
        (excel_col, STATION_MAPPING[excel_col], f"{excel_col}æ—¶é—´")
        for excel_col in STATION_MAPPING.keys()
        if f"{excel_col}æ—¶é—´" in df.columns
    ]

    for _, row in df.iterrows():

        shell_id = row.get('å£³ä½“å·', '')

        if pd.isna(shell_id) or shell_id == '':

            continue

        shell_id = str(shell_id).strip()

        

        part_number_value = row.get('æ–™å·', '')

        if production_order_column is not None and production_order_column in row.index:

            production_order_value = row.get(production_order_column, '')

        else:

            production_order_value = row.get('ç”Ÿäº§è®¢å•', '')

        part_number = "" if pd.isna(part_number_value) else str(part_number_value).strip()

        production_order = "" if pd.isna(production_order_value) else str(production_order_value).strip()

        

        current_station_raw = row.get('å½“å‰ç«™ç‚¹', '')

        current_station = normalize_station_name(current_station_raw)

        

        # æ”¶é›†æœªè¯†åˆ«çš„ç«™åˆ«

        if (

            current_station_raw

            and current_station == current_station_raw

            and current_station not in BASE_STATIONS

            and current_station not in {'å·¥ç¨‹åˆ†æ', 'å·²å®Œæˆ', 'æœªå¼€å§‹'}

        ):

            unrecognized_stations.add(current_station_raw)

        

        shell_progress = {

            'å£³ä½“å·': shell_id,

            'æ–™å·': part_number,

            'ç”Ÿäº§è®¢å•': production_order,

            'å½“å‰ç«™ç‚¹': current_station,

            'å½“å‰ç«™ç‚¹åŸå§‹': current_station_raw,

            'ä¸Šä¸€ç«™': row.get('ä¸Šä¸€ç«™', ''),

            'å®Œæˆç«™åˆ«': [],

            'ç«™åˆ«æ—¶é—´': {},

            'æ˜¯å¦å·¥ç¨‹åˆ†æ': current_station == 'å·¥ç¨‹åˆ†æ'

        }



        station_time_mapping: Dict[str, object] = {}

        


        for _, standard_station, time_col in existing_station_time_cols:

            time_value = row.get(time_col)

            if pd.notna(time_value) and str(time_value).strip():

                shell_progress['å®Œæˆç«™åˆ«'].append(standard_station)

                station_time_mapping[standard_station] = time_value

        

        shell_progress['ç«™åˆ«æ—¶é—´'] = station_time_mapping

        

        progress_data.append(shell_progress)

    

    # å¦‚æœæœ‰æœªè¯†åˆ«çš„ç«™åˆ«ï¼Œæ˜¾ç¤ºè­¦å‘Š

    if unrecognized_stations:

        st.warning(f"âš ï¸ å‘ç°æœªè¯†åˆ«çš„ç«™åˆ«åç§°: {', '.join(sorted(unrecognized_stations))}")

    

    result_df = pd.DataFrame(progress_data)

    result_df.attrs["production_order_column"] = production_order_column

    result_df.attrs["time_cols"] = [f"{excel_col}æ—¶é—´" for excel_col in STATION_MAPPING.keys() if f"{excel_col}æ—¶é—´" in df.columns]

    return result_df



def calculate_station_counts(progress_df: pd.DataFrame) -> pd.DataFrame:

    """ç»Ÿè®¡å„å½“å‰ç«™åˆ«çš„å£³ä½“æ•°é‡ä¸å æ¯”"""

    if progress_df.empty:

        return pd.DataFrame(columns=["ç«™åˆ«", "æ•°é‡", "å æ¯”"])



    unknown_label = "æœªè¯†åˆ«"

    station_series = (

        progress_df["å½“å‰ç«™ç‚¹"]

        .fillna("")

        .astype(str)

        .str.strip()

    )

    station_series = station_series.replace({"": unknown_label, "nan": unknown_label})

    station_series = station_series.apply(

        lambda value: normalize_station_name(value) if value != unknown_label else value

    )



    counts = station_series.value_counts(dropna=False).reset_index()

    counts.columns = ["ç«™åˆ«", "æ•°é‡"]

    counts["å æ¯”"] = counts["æ•°é‡"] / len(progress_df)



    # RMA å·²ç»åœ¨ BASE_STATIONS ä¸­ï¼Œå·¥ç¨‹åˆ†ææ”¾åœ¨æœ€å

    ordered_labels = BASE_STATIONS + ["å·¥ç¨‹åˆ†æ", "å·²å®Œæˆ", unknown_label]

    order_map = {label: idx for idx, label in enumerate(ordered_labels)}

    counts["æ’åº"] = counts["ç«™åˆ«"].map(order_map)



    fallback_order = len(ordered_labels) + counts.index.to_series()

    counts["æ’åº"] = counts["æ’åº"].fillna(fallback_order)



    counts = counts.sort_values(["æ’åº", "ç«™åˆ«"]).drop(columns="æ’åº").reset_index(drop=True)



    return counts



def create_progress_table(progress_df: pd.DataFrame) -> pd.DataFrame:

    """åˆ›å»ºè¿›åº¦è¡¨æ ¼"""

    table_data = []

    

    for _, row in progress_df.iterrows():

        part_number = row.get('æ–™å·', '')

        stations = get_stations_for_part(part_number)

        current_station = row.get('å½“å‰ç«™ç‚¹', '')

        is_engineering = row.get('æ˜¯å¦å·¥ç¨‹åˆ†æ', False)

        

        # å¦‚æœæ˜¯å·¥ç¨‹åˆ†æï¼Œæ·»åŠ å·¥ç¨‹åˆ†æç«™åˆ«

        if is_engineering:

            stations.append('å·¥ç¨‹åˆ†æ')

        

        # è·å–å½“å‰ç«™ç‚¹çš„åºå·ï¼ˆç”¨äºæ’åºå’Œè®¡ç®—å·²å®Œæˆç«™åˆ«æ•°ï¼‰

        station_order = -1

        completed_count = 0

        

        if is_engineering:

            # å·¥ç¨‹åˆ†æï¼šæ ¹æ®ä¸Šä¸€ç«™è®¡ç®—å·²å®Œæˆç«™åˆ«æ•°

            last_station = row.get('ä¸Šä¸€ç«™', '')

            last_station_normalized = normalize_station_name(last_station)

            if last_station_normalized and last_station_normalized in stations:

                station_order = stations.index(last_station_normalized)

                completed_count = station_order + 1  # åŒ…æ‹¬ä¸Šä¸€ç«™

        elif current_station == "å·²å®Œæˆ":

            # å·²å®Œæˆï¼šæ‰€æœ‰ç«™åˆ«éƒ½å·²å®Œæˆ

            station_order = len(stations) - 1  # æœ€åä¸€ä¸ªç«™åˆ«çš„ç´¢å¼•

            completed_count = len(stations)  # æ‰€æœ‰ç«™åˆ«éƒ½å®Œæˆ

        elif current_station and current_station in stations:

            # æ­£å¸¸æƒ…å†µï¼šå½“å‰ç«™ç‚¹ä¹‹å‰çš„éƒ½æ˜¯å·²å®Œæˆ

            station_order = stations.index(current_station)

            completed_count = station_order  # å½“å‰ç«™ç‚¹ä¹‹å‰çš„ç«™åˆ«æ•°

        else:

            # æ²¡æœ‰å½“å‰ç«™ç‚¹ä¿¡æ¯ï¼Œä½¿ç”¨å®Œæˆæ—¶é—´è®°å½•

            completed_count = len(row['å®Œæˆç«™åˆ«'])

        

        total_count = len(stations)

        progress_pct = (completed_count / total_count * 100) if total_count > 0 else 0

        

        # è·å–æœ€æ–°å®Œæˆç«™åˆ«ï¼ˆå½“å‰ç«™ç‚¹çš„å‰ä¸€ä¸ªï¼‰

        last_completed_station = ''

        if station_order > 0:

            last_completed_station = stations[station_order - 1]

        elif row['å®Œæˆç«™åˆ«']:

            last_completed_station = row['å®Œæˆç«™åˆ«'][-1]

        

        table_data.append({

            'å£³ä½“å·': row['å£³ä½“å·'],

            'æ–™å·': part_number,

            'ç”Ÿäº§è®¢å•': row.get('ç”Ÿäº§è®¢å•', ''),

            'å½“å‰ç«™ç‚¹': current_station,

            'å·²å®Œæˆç«™åˆ«æ•°': completed_count,

            'æ€»ç«™åˆ«æ•°': total_count,

            'å®Œæˆè¿›åº¦': f"{progress_pct:.1f}%",

            'æœ€æ–°å®Œæˆç«™åˆ«': last_completed_station,

            'æ˜¯å¦å·¥ç¨‹åˆ†æ': 'æ˜¯' if is_engineering else 'å¦',

            'ç«™åˆ«åºå·': station_order  # ç”¨äºæ’åºçš„éšè—åˆ—

        })

    

    result_df = pd.DataFrame(table_data)
    
    # åªåœ¨DataFrameä¸ä¸ºç©ºæ—¶è¿›è¡Œæ’åºå’Œåˆ é™¤åˆ—æ“ä½œ
    if 'ç«™åˆ«åºå·' in result_df.columns:
        if not result_df.empty:
            # æŒ‰ç«™åˆ«åºå·æ’åºï¼ˆä»å°åˆ°å¤§ï¼Œè¿›åº¦æ…¢çš„åœ¨å‰ï¼‰
            result_df = result_df.sort_values('ç«™åˆ«åºå·', ascending=True)
        # åˆ é™¤æ’åºç”¨çš„åˆ—ï¼›errors='ignore' é˜²æ­¢åˆ—ä¸å­˜åœ¨æ—¶æŠ¥é”™
        result_df = result_df.drop(columns=['ç«™åˆ«åºå·'], errors='ignore')

    

    return result_df



# Streamlit é¡µé¢

st.set_page_config(page_title="æ¨¡å—è¿›åº¦", page_icon="ğŸ“Š", layout="wide")



st.title("æ¨¡å—WIPè¿›åº¦")



st.markdown(

    """

    <style>

    .stMultiSelect div[data-baseweb="select"] > div {

        flex-wrap: wrap;

    }

    .stMultiSelect [data-baseweb="tag"] {

        max-width: none !important;

        min-width: 260px !important;

        width: fit-content !important;

        display: inline-flex !important;

        align-items: center !important;

        justify-content: space-between !important;

        flex: 0 0 auto !important;

        gap: 6px !important;

    }

    .stMultiSelect [data-baseweb="tag"] > * {

        max-width: none !important;

        flex: 1 1 auto !important;

    }

    .stMultiSelect [data-baseweb="tag-text"] {

        max-width: none !important;

        flex: 1 1 auto !important;

    }

    .stMultiSelect [data-baseweb="tag-text"] span {

        max-width: none !important;

        white-space: nowrap !important;

        overflow: visible !important;

        text-overflow: clip !important;

    }

    .stMultiSelect [data-baseweb="tag"] p {

        white-space: nowrap !important;

        overflow: visible !important;

    }

    .custom-order-tags {

        display: flex;

        flex-wrap: wrap;

        gap: 10px;

        margin-top: 6px;

    }

    .custom-order-tag {

        background-color: #ff5a5f;

        color: #ffffff;

        padding: 4px 14px;

        border-radius: 10px;

        font-weight: 600;

        letter-spacing: 0.5px;

    }

    </style>

    """,

    unsafe_allow_html=True,

)



# åˆå§‹åŒ– session_state

if 'progress_df' not in st.session_state:

    st.session_state.progress_df = None

if 'progress_raw_df' not in st.session_state:

    st.session_state.progress_raw_df = None

if 'uploaded_filename' not in st.session_state:

    st.session_state.uploaded_filename = None

if 'progress_dir_cache' not in st.session_state:
    st.session_state.progress_dir_cache = {}
if 'progress_data_cache' not in st.session_state:
    st.session_state.progress_data_cache = {}



# æ·»åŠ æ•°æ®æºé€‰æ‹©ï¼ˆé»˜è®¤ä»æ–‡ä»¶å¤¹é€‰æ‹©ï¼‰

if 'progress_data_source' not in st.session_state:

    st.session_state.progress_data_source = "ğŸ“ ä»æ–‡ä»¶å¤¹é€‰æ‹©"



data_source = st.radio(

    "é€‰æ‹©æ•°æ®æº",

    options=["ğŸ“ ä»æ–‡ä»¶å¤¹é€‰æ‹©", "ğŸ“¤ ä¸Šä¼ æ–‡ä»¶"],

    index=0,  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªï¼ˆä»æ–‡ä»¶å¤¹é€‰æ‹©ï¼‰

    horizontal=True,

    key="progress_data_source"

)



uploaded_file = None

selected_file_path = None



if data_source == "ğŸ“¤ ä¸Šä¼ æ–‡ä»¶":

    # æ–‡ä»¶ä¸Šä¼ 

    uploaded_file = st.file_uploader(

        "ä¸Šä¼ åŒ…å«å£³ä½“è¿›åº¦ä¿¡æ¯çš„æ–‡ä»¶",

        type=['csv', 'xlsx', 'xls'],

        help="è¯·ä¸Šä¼ åŒ…å«å£³ä½“å·å’Œå„ç«™åˆ«æ—¶é—´ä¿¡æ¯çš„ Excel æˆ– CSV æ–‡ä»¶"

    )

    

    # å¦‚æœä¸Šä¼ äº†æ–°æ–‡ä»¶ï¼Œæ›´æ–° session_state

    if uploaded_file is not None and (st.session_state.uploaded_filename != uploaded_file.name):

        # è§£ææ–‡ä»¶å¹¶ä¿å­˜åˆ° session_state

        with st.spinner("æ­£åœ¨è§£ææ–‡ä»¶..."):

            df = parse_uploaded_file(uploaded_file)

        

        if df is not None:

            st.session_state.progress_raw_df = df

            st.session_state.progress_df = extract_progress_data(df, light=st.session_state.get('progress_only_stats', False))

            st.session_state.uploaded_filename = uploaded_file.name

            st.success(f"âœ… æ–‡ä»¶è§£ææˆåŠŸï¼å…± {len(df)} æ¡è®°å½•")



else:  # ä»æ–‡ä»¶å¤¹é€‰æ‹©



    col_path, col_refresh = st.columns([4, 1])

    with col_path:

        folder_path = st.text_input(

            "æ–‡ä»¶å¤¹è·¯å¾„",

            value=DEFAULT_DATA_FOLDER,

            placeholder=f"é»˜è®¤: {DEFAULT_DATA_FOLDER}",

            key="progress_folder_path"

        )

    with col_refresh:

        st.markdown("<div style='margin-top: 32px;'></div>", unsafe_allow_html=True)

        refresh_btn = st.button("ğŸ”„ åˆ·æ–°", use_container_width=True)

    

    if folder_path:

        try:

            search_path = resolve_input_path(folder_path)

            if search_path.exists() and search_path.is_dir():

                # æŸ¥æ‰¾ Excel å’Œ CSV æ–‡ä»¶

                excel_files = list(search_path.glob("*.xlsx")) + list(search_path.glob("*.xls"))

                csv_files = list(search_path.glob("*.csv"))

                all_files = sorted(excel_files + csv_files, key=lambda x: x.stat().st_mtime, reverse=True)

                

                # ç­›é€‰åŒ…å«"å…‰è€¦WIPæŠ¥è¡¨"çš„æ–‡ä»¶

                wip_files = [f for f in all_files if any(keyword in f.name or keyword.lower() in f.name.lower() for keyword in WIP_REPORT_KEYWORDS)]

                

                if all_files:

                    # å¦‚æœæ‰¾åˆ°å…‰è€¦WIPæŠ¥è¡¨æ–‡ä»¶ï¼Œä¼˜å…ˆæ˜¾ç¤º

                    display_files = wip_files if wip_files else all_files
                    MAX_DISPLAY_FILES = 200
                    display_files = display_files[:MAX_DISPLAY_FILES]

                    

                    # åˆ›å»ºæ–‡ä»¶é€‰æ‹©ä¸‹æ‹‰æ¡†

                    _dir_key = str(search_path)
                    _dir_cache = st.session_state.progress_dir_cache.get(_dir_key, {})
                    file_display_map = {}
                    for f in display_files:
                        fp = str(f)
                        mtime = f.stat().st_mtime
                        meta = _dir_cache.get(fp)
                        if not meta or meta.get('mtime') != mtime:
                            size_kb = f.stat().st_size / 1024.0
                            _dir_cache[fp] = {'mtime': mtime, 'size_kb': size_kb}
                        else:
                            size_kb = meta['size_kb']
                        file_display_map[f"{f.name} ({size_kb:.1f} KB)"] = fp
                    st.session_state.progress_dir_cache[_dir_key] = _dir_cache
                    file_options = file_display_map

                    

                    # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶ï¼ˆæœ€æ–°çš„å…‰è€¦WIPæŠ¥è¡¨ï¼‰

                    default_index = 0

                    

                    selected_file_display = st.selectbox(

                        "é€‰æ‹©æ–‡ä»¶" + (" (å·²ç­›é€‰å…‰è€¦WIPæŠ¥è¡¨)" if wip_files else ""),

                        options=list(file_options.keys()),

                        index=default_index,

                        key="progress_file_select"

                    )

                    

                    if selected_file_display:

                        selected_file_path = file_options[selected_file_display]

                        

                        # è‡ªåŠ¨åŠ è½½æœ€æ–°çš„å…‰è€¦WIPæŠ¥è¡¨ï¼ˆä»…åœ¨é¦–æ¬¡åŠ è½½æ—¶ï¼‰

                        auto_load = False

                        if st.session_state.progress_df is None and wip_files and selected_file_display == list(file_options.keys())[0]:

                            auto_load = True

                        

                        # æ·»åŠ åŠ è½½æŒ‰é’®

                        load_btn = st.button("ğŸ“‚ åŠ è½½é€‰ä¸­çš„æ–‡ä»¶", type="primary")

                        

                        if load_btn or auto_load:

                            with st.spinner(f"æ­£åœ¨åŠ è½½ {Path(selected_file_path).name}..."):

                                try:
                                    p = Path(selected_file_path)
                                    _data_key = f"{p.resolve()}::{p.stat().st_mtime}"
                                    cached = st.session_state.progress_data_cache.get(_data_key)
                                    if cached:
                                        df, cached_progress_df = cached
                                        st.session_state.progress_raw_df = df
                                        st.session_state.progress_df = cached_progress_df
                                        st.session_state.uploaded_filename = Path(selected_file_path).name
                                        st.success(f"âœ… å·²ä»ç¼“å­˜åŠ è½½ï¼å…± {len(df)} æ¡è®°å½•")
                                        st.rerun()

                                    read_t0 = time.perf_counter()
                                    if selected_file_path.endswith('.csv'):
                                        header_df = pd.read_csv(selected_file_path, nrows=0)
                                        usecols = _compute_usecols(list(header_df.columns))
                                        time_cols = [f"{excel_col}æ—¶é—´" for excel_col in STATION_MAPPING.keys() if f"{excel_col}æ—¶é—´" in header_df.columns]
                                        dtype_map = {c: "string" for c in ["å£³ä½“å·", "æ–™å·", "ç”Ÿäº§è®¢å•"] if c in usecols}
                                        df = pd.read_csv(selected_file_path, usecols=usecols, dtype=dtype_map, parse_dates=time_cols, infer_datetime_format=True, low_memory=False)
                                    else:
                                        header_df = pd.read_excel(selected_file_path, nrows=0)
                                        usecols = _compute_usecols(list(header_df.columns))
                                        df = pd.read_excel(selected_file_path, usecols=usecols, engine="openpyxl" if selected_file_path.endswith('.xlsx') else None)
                                        time_cols = [f"{excel_col}æ—¶é—´" for excel_col in STATION_MAPPING.keys() if f"{excel_col}æ—¶é—´" in header_df.columns]
                                        if time_cols:
                                            df[time_cols] = df[time_cols].apply(pd.to_datetime, errors='coerce')
                                    read_t1 = time.perf_counter()

                                    

                                    st.session_state.progress_raw_df = df

                                    parse_t0 = time.perf_counter()
                                    st.session_state.progress_df = extract_progress_data(df, light=st.session_state.get('progress_only_stats', False))
                                    parse_t1 = time.perf_counter()
                                    st.info(f"è¯»å–è€—æ—¶: {(read_t1 - read_t0)*1000:.0f} msï¼Œè§£æè€—æ—¶: {(parse_t1 - parse_t0)*1000:.0f} ms")

                                    st.session_state.uploaded_filename = Path(selected_file_path).name

                                    st.success(f"âœ… æ–‡ä»¶åŠ è½½æˆåŠŸï¼å…± {len(df)} æ¡è®°å½•")
                                    try:
                                        p = Path(selected_file_path)
                                        _data_key = f"{p.resolve()}::{p.stat().st_mtime}"
                                        st.session_state.progress_data_cache[_data_key] = (df, st.session_state.progress_df)
                                    except Exception:
                                        pass

                                    if auto_load:

                                        st.rerun()

                                except Exception as e:

                                    st.error(f"æ–‡ä»¶åŠ è½½å¤±è´¥: {str(e)}")

                else:

                    st.warning(f"åœ¨ `{search_path}` ä¸­æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶")

            else:

                st.error(f"è·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯æ–‡ä»¶å¤¹: {search_path}")

        except ValueError as value_error:

            st.error(str(value_error))

        except Exception as e:

            st.error(f"è¯»å–æ–‡ä»¶å¤¹æ—¶å‡ºé”™: {str(e)}")



# ä½¿ç”¨ session_state ä¸­çš„æ•°æ®

if st.session_state.progress_df is not None:

    progress_df = st.session_state.progress_df

    df = st.session_state.progress_raw_df

    # å…¼å®¹æ—§ç‰ˆ session_stateï¼šå¦‚æœç¼ºå°‘ç”Ÿäº§è®¢å•åˆ—æˆ–å±æ€§ï¼Œåˆ™é‡æ–°ç”Ÿæˆ

    if (

        progress_df is not None

        and df is not None

        and (

            'ç”Ÿäº§è®¢å•' not in progress_df.columns

            or "production_order_column" not in progress_df.attrs

        )

    ):

        progress_df = extract_progress_data(df, light=st.session_state.get('progress_only_stats', False))

        st.session_state.progress_df = progress_df

    preview_df = df

    production_order_column = progress_df.attrs.get("production_order_column")

    

    if len(progress_df) > 0:

        filtered_progress_df = progress_df.copy()

        selected_order_values = None

        selected_orders_display: List[str] = []



        with st.container():

            if 'ç”Ÿäº§è®¢å•' in filtered_progress_df.columns:

                order_series = (

                    filtered_progress_df['ç”Ÿäº§è®¢å•']

                    .dropna()

                    .astype(str)

                    .str.strip()

                )

                order_series = order_series[order_series != ""]

                order_options = sorted(order_series.unique().tolist())

                

                if order_options:

                    selected_orders = st.multiselect(

                        "ç”Ÿäº§è®¢å•",

                        options=order_options,

                        default=order_options,

                        key="progress_production_orders",

                    )

                    selected_orders_display = selected_orders or []

                    if selected_orders_display:

                        selected_order_values = {

                            order.strip() for order in selected_orders_display

                        }

                        filtered_progress_df = filtered_progress_df[

                            filtered_progress_df['ç”Ÿäº§è®¢å•']

                            .fillna("")

                            .astype(str)

                            .str.strip()

                            .isin(selected_order_values)

                        ]

                    else:

                        selected_order_values = set()

                        filtered_progress_df = filtered_progress_df.iloc[0:0]

                else:

                    st.multiselect(

                        "ç”Ÿäº§è®¢å•",

                        options=[],

                        default=[],

                        key="progress_production_orders",

                    )

                    st.caption("æœªæ£€æµ‹åˆ°ç”Ÿäº§è®¢å•æ•°æ®")

            else:

                st.info("å½“å‰æ•°æ®ç¼ºå°‘ç”Ÿäº§è®¢å•åˆ—")

        

        if selected_order_values is not None:

            if production_order_column and df is not None and production_order_column in df.columns:

                preview_series = (

                    df[production_order_column]

                    .fillna("")

                    .astype(str)

                    .str.strip()

                )

                if selected_order_values:

                    preview_df = df[preview_series.isin(selected_order_values)]

                else:

                    preview_df = df.iloc[0:0]

            elif not selected_order_values:

                preview_df = df.iloc[0:0]

        

        if filtered_progress_df.empty:

            st.warning("ç­›é€‰æ¡ä»¶ä¸‹æ²¡æœ‰æ•°æ®ï¼Œè¯·è°ƒæ•´ç”Ÿäº§è®¢å•é€‰æ‹©ã€‚")

        else:

            st.caption('å½“å‰ç­›é€‰ç»“æœå·²ç¼“å­˜ï¼Œå¯åœ¨â€œæ•°æ®åˆ†æâ€é¡µç»Ÿä¸€ä¿å­˜ã€‚')



            # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯

            col1, col2, col3, col4 = st.columns([1, 1.2, 1, 1.5])

            with col1:

                st.metric("å£³ä½“æ€»æ•°", len(filtered_progress_df))

            with col2:
                if 'å®Œæˆç«™åˆ«' in filtered_progress_df.columns:
                    avg_progress = filtered_progress_df['å®Œæˆç«™åˆ«'].apply(len).mean()
                    st.metric("å¹³å‡å®Œæˆç«™åˆ«æ•°", f"{avg_progress:.1f}")

            with col3:

                total_stations = len(BASE_STATIONS)

                st.metric("åŸºç¡€ç«™åˆ«æ•°", total_stations)

            with col4:
                latest_time = None
                time_cols = progress_df.attrs.get("time_cols", [])
                if df is not None and time_cols:
                    tc = [c for c in time_cols if c in df.columns]
                    if tc:
                        parsed = df[tc].apply(pd.to_datetime, errors='coerce')
                        max_val = parsed.max().max()
                        if pd.notna(max_val):
                            latest_time = max_val
                if latest_time:
                    st.metric("æœ€æ–°æµ‹è¯•æ—¶é—´", latest_time.strftime("%Y-%m-%d %H:%M"))
                else:
                    st.metric("æœ€æ–°æµ‹è¯•æ—¶é—´", "æ— æ•°æ®")

            

            counts_df = calculate_station_counts(filtered_progress_df)

            if not counts_df.empty:

                st.markdown("---")

                st.markdown("### å„ç«™åˆ«å½“å‰æ•°é‡")

                table_col, chart_col = st.columns([2, 3])

                with table_col:

                    counts_style = counts_df.style.format({"å æ¯”": "{:.1%}"})

                    table_height = max(180, min(320, 36 * len(counts_df) + 60))

                    st.dataframe(counts_style, use_container_width=True, height=table_height)

                with chart_col:

                    station_order = counts_df["ç«™åˆ«"].tolist()

                    chart_height = max(160, min(360, 28 * len(counts_df)))

                    # æ·»åŠ é¢œè‰²æ˜ å°„ä»¥åˆ›å»ºæ¸å˜æ•ˆæœ
                    chart = (

                        alt.Chart(counts_df)

                        .mark_bar(
                            cornerRadius=8,  # åœ†è§’æ•ˆæœ
                            opacity=0.9,     # ç•¥å¾®é€æ˜å¢åŠ å±‚æ¬¡æ„Ÿ
                            strokeWidth=1.5  # æè¾¹å®½åº¦
                        )

                        .encode(

                            x=alt.X("æ•°é‡:Q", title="å®Œæˆæ•°é‡", 
                                    axis=alt.Axis(grid=True, gridOpacity=0.2, tickMinStep=1)),

                            y=alt.Y("ç«™åˆ«:N", sort=station_order, title="ç«™åˆ«",
                                    axis=alt.Axis(labelFontSize=12, labelFontWeight='bold')),

                            # ä½¿ç”¨æ¸å˜è‰²æ–¹æ¡ˆåˆ›å»º3Dæ„Ÿ
                            color=alt.Color('æ•°é‡:Q',
                                          scale=alt.Scale(
                                              scheme='blues',  # è“è‰²æ¸å˜æ–¹æ¡ˆ
                                              domain=[counts_df["æ•°é‡"].min(), counts_df["æ•°é‡"].max()]
                                          ),
                                          legend=None),

                            # æ·»åŠ æè¾¹é¢œè‰²ï¼Œè®©æ¡å½¢æ›´ç«‹ä½“
                            stroke=alt.value('#ffffff33'),  # åŠé€æ˜ç™½è‰²æè¾¹

                            tooltip=["ç«™åˆ«", "æ•°é‡", alt.Tooltip("å æ¯”:Q", title="å æ¯”", format=".1%")],

                        )

                    ).properties(
                        height=chart_height
                    ).configure_view(
                        strokeWidth=0  # ç§»é™¤å¤–è¾¹æ¡†
                    ).configure_axis(
                        titleFontSize=13,
                        titleFontWeight='bold'
                    )

                    st.altair_chart(chart, use_container_width=True, theme="streamlit")

            

            # å·¥ç¨‹åˆ†æç«™åˆ«åˆ†å¸ƒ

            engineering_df = filtered_progress_df[filtered_progress_df['æ˜¯å¦å·¥ç¨‹åˆ†æ'] == True]

            if not engineering_df.empty:

                st.markdown("---")

                st.markdown("### ğŸ” å·¥ç¨‹åˆ†æç«™åˆ«åˆ†å¸ƒ")

                

                # ç»Ÿè®¡å·¥ç¨‹åˆ†æåœ¨å„ç«™åˆ«çš„æ•°é‡

                engineering_stations = []

                for _, row in engineering_df.iterrows():

                    last_station = row.get('ä¸Šä¸€ç«™', '')

                    last_station_normalized = normalize_station_name(last_station)

                    if last_station_normalized:

                        engineering_stations.append(last_station_normalized)

                

                if engineering_stations:

                    engineering_counts = pd.Series(engineering_stations).value_counts().reset_index()

                    engineering_counts.columns = ['ç«™åˆ«', 'æ•°é‡']

                    engineering_counts['å æ¯”'] = engineering_counts['æ•°é‡'] / engineering_counts['æ•°é‡'].sum()

                    

                    eng_table_col, eng_chart_col = st.columns([2, 3])

                    

                    with eng_table_col:

                        st.caption(f"å·¥ç¨‹åˆ†ææ€»æ•°: {len(engineering_df)} ä¸ª")

                        eng_counts_style = engineering_counts.style.format({"å æ¯”": "{:.1%}"})

                        st.dataframe(eng_counts_style, use_container_width=True, height=300)

                    

                    with eng_chart_col:

                        # åˆ›å»ºé¥¼å›¾

                        pie_chart = alt.Chart(engineering_counts).mark_arc(innerRadius=40).encode(

                            theta=alt.Theta('æ•°é‡:Q', stack=True),

                            color=alt.Color('ç«™åˆ«:N', 

                                          legend=alt.Legend(title='ç«™åˆ«', orient='right'),

                                          scale=alt.Scale(scheme='category20')),

                            tooltip=[

                                alt.Tooltip('ç«™åˆ«:N', title='ç«™åˆ«'),

                                alt.Tooltip('æ•°é‡:Q', title='æ•°é‡'),

                                alt.Tooltip('å æ¯”:Q', title='å æ¯”', format='.1%')

                            ]

                        ).properties(

                            height=300,

                            title='å·¥ç¨‹åˆ†æç«™åˆ«å æ¯”'

                        )

                        st.altair_chart(pie_chart, use_container_width=True)

            

            if not st.session_state.get('progress_only_stats', False):
                st.markdown("---")
                st.markdown("### ğŸ“‹ è¿›åº¦è¡¨æ ¼")
                show_eng_only = st.checkbox("ğŸ” ä»…æ˜¾ç¤ºå·¥ç¨‹åˆ†æçš„å£³ä½“", value=False, key="progress_show_eng_only")
                source_df = filtered_progress_df[filtered_progress_df['æ˜¯å¦å·¥ç¨‹åˆ†æ'] == True] if show_eng_only else filtered_progress_df
                table_df = create_progress_table(source_df)
                def highlight_engineering(row):
                    return [''] * len(row)
                styled_df = table_df.style.apply(highlight_engineering, axis=1)
                st.dataframe(styled_df, use_container_width=True, height=400)







    else:

        st.warning("âš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„å£³ä½“è¿›åº¦æ•°æ®")

        

    # æ˜¾ç¤ºåŸå§‹æ•°æ®é¢„è§ˆ

    with st.expander("ğŸ“„ æŸ¥çœ‹åŸå§‹æ•°æ®"):

        st.dataframe(preview_df.head(20), use_container_width=True)

else:

    # æ˜¾ç¤ºä½¿ç”¨è¯´æ˜

    st.info("""

    ### ğŸ“– ä½¿ç”¨è¯´æ˜

    

    1. **ä¸Šä¼ æ–‡ä»¶**ï¼šç‚¹å‡»ä¸Šæ–¹æŒ‰é’®ä¸Šä¼ åŒ…å«å£³ä½“è¿›åº¦ä¿¡æ¯çš„ Excel æˆ– CSV æ–‡ä»¶

    2. **æŸ¥çœ‹ç»“æœ**ï¼š

       - ç”˜ç‰¹å›¾ï¼šç›´è§‚å±•ç¤ºæ‰€æœ‰å£³ä½“åœ¨å„ç«™åˆ«çš„è¿›åº¦

       - è¿›åº¦è¡¨æ ¼ï¼šè¯¦ç»†åˆ—å‡ºæ¯ä¸ªå£³ä½“çš„å®Œæˆæƒ…å†µ

    """)

if 'progress_only_stats' not in st.session_state:
    st.session_state.progress_only_stats = False
st.checkbox("ä»…ç»Ÿè®¡æ¨¡å¼", value=st.session_state.progress_only_stats, key="progress_only_stats")

